{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sonder-art/mhar/blob/released/nbs/tutorial_full_dimensional.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the next snippet if you are in colab or need to install mhar library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mhar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will show how to use `mhar` for sampling full dimensional polytopes. It is focused on executing parallel MCMC walks over a polytope in GPUs. If you want to se a tutorial on how to sample non-full dimensional polytopes see [tutorial](https://github.com/sonder-art/mhar/blob/released/nbs/tutorial_nonfull_dimensional.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting let's check if you have an avaialble gpu device or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to decide the data-type `dtype` we are going to use. Depending on your necessities you can choose it, we recomend to use `64` bits for non-fully dimentional polytopes in order to maintain numerical inestability of the projections. Otherwiise the precision depends on the dimension of your polytope and speed you want.  \n",
    "  \n",
    "As of now `16` bit precision is only available for `gpu` and not `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose 64-bits\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polytope in question must be presented in matrix canonical representation (as opposed to vertex). `mhar` assumes that the matrix has no repeated or redundant restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully dimensional Polytopes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $A^IX \\leq b^I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fully dimensional polytopes we need to use the class `Polytope` in the `mhar.polytope` module. The restrictions must be passed as pytorch tensors.  \n",
    "  \n",
    "We will sample the unit hypercube that is defined as:  \n",
    "> $n-hypercube = \\{x \\in R^n || x \\in [-1,1]^n \\} $  \n",
    "\n",
    "Which we can represent in matrix restrictions:  \n",
    "$ Ix \\leq 1$  \n",
    "$ -Ix \\leq 1$  \n",
    "Where $I$ is the identity matrix of dimension $n \\times n$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this restrictions to define the polytope as:  \n",
    "$A^Ix = [I | -I]x \\leq 1 = b^I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the tensors to represent the restrictions that define the polytope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inequality Matrix A^I \n",
      " tensor([[ 1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [-1., -0., -0.],\n",
      "        [-0., -1., -0.],\n",
      "        [-0., -0., -1.]]) \n",
      "\n",
      "Inequality Vector b^I \n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "n = 3 # Dimension\n",
    "dtype = torch.float32 # Precision \n",
    "A_I = torch.cat((torch.eye(n), torch.eye(n) * -1.0), dim=0).to(dtype) # Inequality Matrix\n",
    "b_I = torch.ones(2 * n, dtype=dtype).view(-1, 1)  # Inequality restriction vector      \n",
    "print(f'Inequality Matrix A^I \\n {A_I} \\n')\n",
    "print(f'Inequality Vector b^I \\n {b_I}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a `Polytope` object to represent the polytope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder-art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope import Polytope\n",
    "hypercube = Polytope(A_I, # Inequality Restriction Matrix \n",
    "                     b_I,  # Inequality Vector\n",
    "                     dtype, # torch dtype\n",
    "                     device, # device used cpu or cuda\n",
    "                     copy=False # bool for creating a copy of the restrictions\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Numeric Precision (dtype) torch.float32\n",
       "Device: cuda\n",
       "A_in: torch.Size([6, 3]) \n",
       "b_in: torch.Size([6, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypercube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Inner Point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start the algorithm we need at least one inner point $x_0$. If you know your inner point you can supply it to the algorithm, `mhar` also contains functions to compute one inner point using the [chebyshev center](https://en.wikipedia.org/wiki/Chebyshev_center) which finds the center of the smallest ball inside the polytope.\n",
    "\n",
    " `from mhar.inner_point import ChebyshevCenter`. The solver is in numpy so precision must be specified as `numpy.dtype`. It uses `linprog` from `scipy.optimize`. You can see the documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html). \n",
    "  \n",
    "It could also be the last points produced by a previous walk/run of the `mhar` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhar.inner_point import ChebyshevCenter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n"
     ]
    }
   ],
   "source": [
    "x0 = ChebyshevCenter(polytope=hypercube, # Polytope Object\n",
    "                    lb=None,  # Lowerbound (lb <= x ), if unknown leave it as None \n",
    "                    ub=None,  # Upperbound ( x <= up), if unknown leave it as None \n",
    "                    tolerance=1e-4, # Tolerance for equality restrictions (A_eqx = b_eq)\n",
    "                    device=device, # device used cpu or cuda\n",
    "                    solver_precision=np.float32 # numpy dtype\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.],\n",
       "        [-0.],\n",
       "        [-0.]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to manually input the inner points then it is enough to use a torch tensor of size $n \\times l$. Where $l$ is ne number of inner points you want to supply. Just write them in column notation.  \n",
    "  \n",
    "We are going to manually add an other starting point to the one calcualted by the `chebyshev center` to show its functionality later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, 0.5000],\n",
       "        [-0.0000, 0.5000],\n",
       "        [-0.0000, 0.5000]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.cat([x0, \n",
    "            torch.tensor([[.5], [.5], [.5]]).to(device).to(dtype)\n",
    "             ], dim=1)\n",
    "x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to sample the `polytope`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to sample the polytope starting from the inner points we supply using the method `walk.walk`. It has the next arguments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `polytope` is an object of the type `Polytope` or `NFDPolytope` that defines it.\n",
    "+ `X0` a tensor containing the inner points to start the walks from.\n",
    "+ `z` determines the number of simoultaneous `walks`. If the number of initial points supplied are less than `z`  ($ncols($ `x0` $) < $ `z`) then some points will be reused as starting points.  \n",
    "+ `T` is the number of uncorrelated iterations you want. The number of total uncorrelated points produced by the algorithm is `z` $\\times$ `T`, since `z` points are sampled at each iteration.  \n",
    "+ `thinning` determines the number of points that we need to burn between iterations in order to get uncorrelated points. The suggested factor should be in the order of $O(n^3)$.\n",
    "+ `warm` determines a thinning for warming the walks only at the beggining, after the this war the walks resumes as normal. It is used if you want to lose the dependency from the starting points.\n",
    "+ `device` device where the tenros live `cpu` or `cuda`\n",
    "+ `seed` for reproducibility\n",
    "+ `verbosity` for printing what is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number allowed -3.4028234663852886e+38\n",
      "Maximum number allowed 3.4028234663852886e+38\n",
      "Eps:  1.1920928955078125e-07\n",
      "Values close to zero will be converted to 3eps or -3eps: 3.5762786865234375e-07\n",
      "n:  3   mI: 6   mE: None   z: 100\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    }
   ],
   "source": [
    "from mhar.walk import walk\n",
    "X = walk(polytope=hypercube,\n",
    "        X0 = x0,  \n",
    "        z=100, \n",
    "        T=1, \n",
    "        warm=0,\n",
    "        thinning=3**3, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`walk` produces `T` $\\times$ `z` uncorrelated points. It returns a vector of dimension `T` $\\times$ `z` $\\times$ `n`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6173,  0.4008, -0.4735, -0.7712, -0.3196,  0.1591,  0.9762,\n",
       "          -0.7801,  0.3353, -0.0524, -0.9032,  0.8997, -0.3554,  0.4499,\n",
       "           0.1636,  0.5015, -0.1343, -0.5433,  0.3641, -0.4847, -0.9843,\n",
       "           0.8414, -0.3623,  0.0414,  0.8504, -0.2147, -0.7649,  0.6023,\n",
       "          -0.3847,  0.2938,  0.9780,  0.6270,  0.9071, -0.7047,  0.9479,\n",
       "          -0.8610,  0.3039,  0.1934,  0.9549,  0.8189,  0.8731,  0.7472,\n",
       "          -0.4261,  0.1306, -0.5324,  0.5551, -0.9605, -0.9427, -0.3306,\n",
       "          -0.8877, -0.6914,  0.9730, -0.7891, -0.4883,  0.4326, -0.9502,\n",
       "          -0.5412,  0.6529,  0.5179, -0.2092, -0.6117,  0.2445,  0.7106,\n",
       "          -0.2785, -0.8416, -0.1208, -0.0635, -0.1734,  0.6887,  0.4285,\n",
       "           0.9015,  0.6880, -0.4168, -0.5558,  0.3563, -0.2213,  0.5438,\n",
       "          -0.1436, -0.7146, -0.1307,  0.3279, -0.4997, -0.0271,  0.9761,\n",
       "           0.3781, -0.3052,  0.4314,  0.6846, -0.5006,  0.1158, -0.9094,\n",
       "           0.7462,  0.3971,  0.2224,  0.0247,  0.7203, -0.6548, -0.8989,\n",
       "           0.9298, -0.4684],\n",
       "         [ 0.0579,  0.2605, -0.8446,  0.8260,  0.5103,  0.2927, -0.1231,\n",
       "           0.5779, -0.2484,  0.8817,  0.5244, -0.2988, -0.3809, -0.5174,\n",
       "          -0.9725, -0.6846,  0.6579, -0.4897, -0.5790, -0.0697, -0.3842,\n",
       "           0.8380,  0.6115,  0.1053,  0.6073, -0.5803,  0.6635, -0.9586,\n",
       "          -0.8021,  0.2132,  0.9007, -0.0230, -0.0903,  0.6844, -0.5026,\n",
       "          -0.4456, -0.0549, -0.5904, -0.9204,  0.3886,  0.6336,  0.4508,\n",
       "           0.9016,  0.0120, -0.4966,  0.4073,  0.7998, -0.2422,  0.0896,\n",
       "          -0.9611,  0.1006, -0.3869,  0.7710,  0.1963, -0.3900, -0.6670,\n",
       "          -0.4697,  0.2962,  0.1652,  0.3750,  0.7633,  0.2290,  0.0995,\n",
       "          -0.2207,  0.8886, -0.2994,  0.5516,  0.8411,  0.9101, -0.7066,\n",
       "           0.3394, -0.2641, -0.9987, -0.7621, -0.4819,  0.1147, -0.2932,\n",
       "          -0.7398,  0.1153,  0.4260, -0.7389,  0.3751,  0.7711, -0.3377,\n",
       "          -0.2568,  0.7119,  0.3663,  0.3686, -0.2528, -0.8485, -0.9499,\n",
       "           0.2991,  0.4618,  0.4263, -0.7362, -0.6627,  0.6528, -0.7472,\n",
       "           0.4933,  0.1326],\n",
       "         [-0.7105, -0.9746,  0.8436,  0.6179, -0.9803,  0.4767, -0.3318,\n",
       "           0.4460,  0.6018,  0.6542, -0.3731, -0.4641, -0.8005,  0.4192,\n",
       "          -0.9974,  0.9199,  0.2174,  0.2539,  0.7376,  0.5437, -0.5672,\n",
       "          -0.4456,  0.0377,  0.5646,  0.9768, -0.6643,  0.3482, -0.5780,\n",
       "           0.3123, -0.2609,  0.2990, -0.0834, -0.7173,  0.3548, -0.0631,\n",
       "          -0.0570, -0.0947, -0.2947, -0.6098, -0.9733, -0.8611,  0.9393,\n",
       "           0.2205,  0.8710, -0.8195,  0.6190, -0.0052,  0.7706, -0.7102,\n",
       "           0.3012,  0.2604,  0.8798, -0.1046,  0.4832, -0.7747, -0.0434,\n",
       "          -0.6136, -0.4489, -0.7153,  0.7438, -0.2074,  0.3065, -0.8645,\n",
       "           0.0465, -0.8419, -0.8125,  0.2811, -0.2840,  0.7535,  0.5502,\n",
       "           0.3633, -0.3806,  0.1020, -0.4313,  0.7812,  0.3731, -0.8655,\n",
       "          -0.0140, -0.2342,  0.9121, -0.8542,  0.0512,  0.2069,  0.7795,\n",
       "           0.3403,  0.2750, -0.0408, -0.6583,  0.8637, -0.8597, -0.3915,\n",
       "           0.8652, -0.5795, -0.7961,  0.6523,  0.7558,  0.3719,  0.7561,\n",
       "          -0.9916, -0.9981]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sumamrize the steps taken we can use the `polytope_examples` for creating a `Hypercube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder-art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope_examples import Hypercube\n",
    "\n",
    "# Create a polytope (Hypercube)\n",
    "hypercube_sim = Hypercube(10,\n",
    "                      dtype=torch.float32,\n",
    "                      device=device\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define/Find inner points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n"
     ]
    }
   ],
   "source": [
    "x0_sim = ChebyshevCenter(polytope=hypercube_sim, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-4,\n",
    "                    device=device,\n",
    "                    solver_precision=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number allowed -3.4028234663852886e+38\n",
      "Maximum number allowed 3.4028234663852886e+38\n",
      "Eps:  1.1920928955078125e-07\n",
      "Values close to zero will be converted to 3eps or -3eps: 3.5762786865234375e-07\n",
      "n:  10   mI: 20   mE: None   z: 100\n",
      "% of burned samples |██----------------------------| 8.9%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9521e-01, -4.9613e-01,  9.4348e-01, -3.6091e-01, -2.1067e-01,\n",
       "          -7.1501e-01,  6.1336e-01,  3.6261e-02, -9.5879e-01,  3.6196e-01,\n",
       "           8.0419e-01, -6.0833e-01,  1.5020e-01, -5.8299e-01, -5.3218e-01,\n",
       "           8.4844e-01,  7.6787e-01,  5.5885e-01, -7.2924e-01, -4.7147e-01,\n",
       "           8.8063e-01, -8.8127e-01,  9.3965e-01,  6.9480e-01,  6.9380e-01,\n",
       "           5.0325e-01, -6.5507e-01, -4.2953e-01, -6.4428e-01,  8.9482e-01,\n",
       "           4.1831e-01,  1.9657e-01,  7.5528e-01,  1.0310e-01,  8.9719e-01,\n",
       "           3.9713e-01,  8.4115e-01, -9.9127e-02, -9.9114e-03,  9.1147e-01,\n",
       "           9.5645e-01,  6.5732e-01,  2.0378e-01, -3.4835e-01,  3.4385e-01,\n",
       "          -4.2524e-01, -3.3002e-01, -1.7727e-01,  6.3531e-01, -3.3323e-01,\n",
       "          -8.0515e-01, -3.0070e-01,  4.8017e-01, -8.3783e-01, -4.3488e-03,\n",
       "           2.6925e-02, -6.7629e-01, -5.8099e-01, -2.0629e-01, -5.2587e-02,\n",
       "          -3.8194e-01,  6.7568e-02,  9.5511e-01, -9.9408e-01,  4.3823e-01,\n",
       "           9.9417e-01, -6.8410e-01,  6.9961e-01,  2.1290e-01, -9.1016e-01,\n",
       "          -4.7265e-01,  6.5303e-01,  2.7085e-01, -2.4136e-02, -3.0732e-01,\n",
       "           5.0453e-01,  1.6233e-01,  6.8291e-01, -1.8932e-01, -5.2796e-01,\n",
       "          -4.6864e-01,  9.0492e-01,  2.9939e-01,  2.2224e-01,  7.6433e-01,\n",
       "          -3.2203e-01,  6.2036e-01, -3.4767e-02,  6.9104e-01,  3.4126e-01,\n",
       "          -9.8035e-01, -7.7383e-01,  5.7021e-01,  6.2976e-01,  9.9824e-01,\n",
       "          -7.4306e-02,  2.7499e-01, -1.8180e-01,  6.3697e-01,  1.6699e-01],\n",
       "         [-3.5882e-02, -4.5396e-01, -3.6075e-01,  2.3089e-01, -6.0428e-01,\n",
       "           3.8184e-01,  8.1325e-01, -7.9001e-01,  8.5250e-01, -8.8013e-01,\n",
       "          -9.6209e-01, -8.5399e-01, -9.2155e-01, -4.5697e-01, -2.6445e-01,\n",
       "           7.0840e-01,  4.0828e-01,  2.9230e-01,  6.1364e-02,  6.2932e-01,\n",
       "           5.8531e-01,  4.1498e-01, -1.6392e-01, -5.6921e-01, -7.3302e-01,\n",
       "          -8.3771e-01, -8.1268e-01, -2.3895e-01, -4.4905e-01, -2.1847e-01,\n",
       "           6.3116e-01,  8.2472e-01, -7.4715e-01,  7.0097e-01,  9.2393e-01,\n",
       "           4.5396e-01, -9.8325e-01, -2.6875e-01,  4.6365e-01, -7.8588e-01,\n",
       "          -5.4172e-02,  8.6843e-01, -6.5043e-01,  5.3143e-01,  1.2920e-01,\n",
       "           8.6867e-01, -7.1652e-01,  4.3560e-01,  9.0976e-01, -4.8883e-01,\n",
       "           4.9880e-01, -8.0724e-01,  9.9037e-01,  3.1715e-01,  2.0550e-01,\n",
       "          -7.0830e-01,  2.0305e-01, -5.4023e-01,  5.0713e-01,  3.9176e-01,\n",
       "          -5.6130e-01,  6.9622e-02, -4.6223e-01,  9.6433e-02, -2.7424e-01,\n",
       "          -2.4874e-01, -4.3128e-01,  3.5794e-01, -3.9055e-01,  8.6089e-01,\n",
       "           3.8646e-01, -7.5446e-01, -6.2037e-01, -6.6759e-02, -6.3131e-01,\n",
       "          -4.7142e-01,  2.4159e-01,  4.0862e-01, -9.1713e-01, -5.4535e-01,\n",
       "          -2.9729e-01, -3.6590e-01,  9.3309e-01, -1.1103e-01, -8.7648e-01,\n",
       "          -6.6767e-01, -7.1322e-02, -7.4786e-01, -5.3624e-01, -7.9818e-01,\n",
       "          -8.5824e-01,  2.9625e-01,  2.1787e-01,  8.5053e-01,  1.8028e-01,\n",
       "          -4.6459e-01,  7.4727e-01,  6.5475e-01,  8.8258e-01,  3.9282e-01],\n",
       "         [-1.4359e-01,  5.2083e-01, -6.4993e-01, -4.1729e-01, -9.3308e-01,\n",
       "          -8.3357e-01, -6.2537e-02, -8.3073e-01, -6.7772e-01, -9.3488e-01,\n",
       "          -5.8229e-01,  2.0255e-01, -5.9180e-01, -4.4643e-01,  4.6055e-01,\n",
       "          -2.8349e-02,  8.5079e-01, -7.0427e-01, -3.3283e-01,  7.8809e-01,\n",
       "           3.8202e-02,  2.6037e-01,  9.2253e-01, -4.1139e-01,  1.3806e-01,\n",
       "          -6.7581e-02, -9.1078e-01,  7.2002e-01,  7.8210e-01,  1.7686e-01,\n",
       "          -2.3654e-01,  5.7321e-01, -7.8520e-01,  1.4317e-01, -7.3938e-01,\n",
       "          -5.1427e-03, -2.7666e-01, -6.5286e-01, -3.1554e-01,  5.0688e-01,\n",
       "           2.4741e-01, -5.0474e-01, -2.8702e-01, -9.8399e-01,  8.5365e-02,\n",
       "          -3.9366e-01, -3.6101e-01,  1.4115e-01, -8.4672e-01, -8.0294e-01,\n",
       "          -3.9238e-01, -3.8276e-01, -1.8905e-01,  8.1017e-01,  8.6014e-03,\n",
       "           4.6040e-01, -6.9734e-01, -6.6114e-01, -7.2382e-01,  1.0009e-01,\n",
       "          -7.5762e-02, -3.5994e-01, -4.1663e-01,  6.3000e-02,  2.0283e-01,\n",
       "          -6.2872e-01,  2.3334e-02, -8.9560e-01, -7.6743e-01,  4.2692e-01,\n",
       "          -5.9037e-04, -1.1988e-01, -7.9535e-02, -6.3024e-01, -1.7193e-01,\n",
       "          -4.9459e-01,  8.8057e-01,  5.8569e-01, -4.1729e-01,  5.0716e-01,\n",
       "           8.1809e-01, -3.7744e-01, -7.7157e-01, -9.6139e-01, -9.9644e-01,\n",
       "          -6.5946e-01,  9.9923e-01, -2.8781e-01,  7.2340e-01, -5.2163e-01,\n",
       "           7.5540e-01, -3.5093e-01,  2.9456e-01, -6.4331e-01,  3.3545e-01,\n",
       "          -7.0925e-01, -9.8747e-01, -3.7278e-01, -9.5963e-01,  4.3259e-02],\n",
       "         [ 7.4706e-01,  5.8671e-01,  5.2887e-02, -2.6283e-01, -6.2417e-01,\n",
       "          -9.4600e-01, -3.2962e-01, -7.7983e-01, -9.9745e-02,  4.2743e-01,\n",
       "          -5.1513e-01,  3.7840e-01, -5.8766e-01, -2.1579e-01, -3.0039e-01,\n",
       "           4.9175e-01,  5.7972e-01, -5.3461e-01,  3.8916e-02,  4.7643e-01,\n",
       "           5.6865e-01, -6.1872e-01,  3.6799e-01, -8.8565e-01, -3.9951e-01,\n",
       "           6.5715e-01, -6.0130e-01, -1.0777e-01, -3.9770e-01, -2.9926e-01,\n",
       "          -3.5198e-01,  1.2062e-02, -3.2937e-02,  1.5793e-01,  9.1714e-01,\n",
       "          -6.3175e-01,  5.6411e-01,  9.0395e-01,  9.6758e-01,  1.6971e-01,\n",
       "          -9.4239e-01, -1.5771e-01, -1.7160e-01,  4.7537e-01,  4.9872e-01,\n",
       "           4.5540e-02,  7.1918e-02,  1.8178e-01, -8.2354e-02, -5.3240e-01,\n",
       "           1.1974e-01,  9.7562e-01, -8.5955e-03,  7.0325e-01, -7.7725e-01,\n",
       "          -4.1452e-01,  4.5520e-01, -8.7559e-02,  3.3939e-01,  4.6835e-01,\n",
       "           2.9628e-01, -1.6136e-01,  4.4899e-01,  9.3466e-01, -1.9361e-01,\n",
       "          -6.0129e-01, -3.1676e-01,  7.8284e-01, -5.1872e-01, -8.4902e-01,\n",
       "           5.7581e-01, -7.5364e-01, -9.3514e-01,  4.2381e-01, -3.4082e-01,\n",
       "          -8.7328e-01,  8.3513e-01,  1.9908e-01,  5.0094e-01, -7.3929e-01,\n",
       "           1.9308e-01, -1.0125e-01,  4.0509e-01,  4.4960e-01, -8.6923e-02,\n",
       "           7.8821e-01, -1.5983e-01,  9.6598e-01,  9.7415e-02, -2.1199e-01,\n",
       "          -4.4614e-01,  1.9837e-01,  5.9499e-01,  7.4652e-01,  4.2696e-01,\n",
       "          -1.0550e-01,  2.2087e-01,  4.2941e-01,  4.9838e-01, -9.6658e-01],\n",
       "         [-3.9297e-01, -8.7984e-01, -7.0999e-01, -1.8773e-01, -2.9588e-01,\n",
       "           7.6069e-01,  9.3431e-01, -5.9316e-01, -6.9615e-01,  5.9086e-01,\n",
       "          -1.4043e-01, -4.8955e-01, -6.3953e-01,  9.0132e-01,  1.5078e-01,\n",
       "           5.3632e-01, -5.8089e-01, -4.2048e-02, -3.9478e-01,  7.5120e-01,\n",
       "           2.0625e-01, -7.6784e-01,  5.5267e-01,  7.3514e-01, -5.4901e-01,\n",
       "           7.0473e-01, -2.2254e-01,  8.1520e-01, -4.1168e-01, -3.9447e-01,\n",
       "           8.0895e-01, -2.2873e-01,  2.3496e-01, -7.5556e-01, -1.5729e-01,\n",
       "          -7.7959e-01,  4.9921e-01,  6.3790e-01,  3.2777e-01,  8.9647e-01,\n",
       "          -5.8105e-01, -4.1734e-01,  5.1398e-01,  3.9458e-01, -4.8143e-01,\n",
       "           7.6675e-01, -6.7661e-01,  4.7541e-01, -9.5844e-01, -2.2170e-01,\n",
       "          -3.6918e-01, -1.5671e-01,  5.9082e-01,  6.7067e-01, -8.5978e-01,\n",
       "           6.8653e-01,  8.3447e-01, -2.7988e-01,  4.2264e-01, -8.0774e-01,\n",
       "          -6.6947e-01, -4.1665e-01,  1.2364e-01, -7.3288e-01,  5.8872e-01,\n",
       "           7.0418e-01,  5.6196e-01, -5.3266e-01,  1.4513e-01, -7.0861e-01,\n",
       "          -5.4928e-02, -9.5148e-01, -7.2514e-01,  1.6630e-01,  6.5228e-01,\n",
       "          -8.8117e-01, -3.7931e-01,  6.9784e-01,  2.1330e-01, -2.6679e-02,\n",
       "           2.1140e-01,  7.8870e-01, -7.2344e-01,  7.1164e-01, -5.9490e-01,\n",
       "           1.3862e-01,  4.4312e-01,  7.3229e-01, -2.1809e-01,  7.8363e-01,\n",
       "          -3.7820e-01,  5.6249e-01,  6.6583e-01,  8.8596e-01, -8.1946e-01,\n",
       "           4.4549e-01, -7.3623e-01,  7.6785e-01, -6.3642e-02,  8.2718e-01],\n",
       "         [-9.9161e-01,  8.7922e-01,  3.3392e-01,  9.8767e-01, -9.5198e-01,\n",
       "          -2.3551e-01,  5.0750e-01, -5.7444e-01, -4.6983e-01, -1.9848e-01,\n",
       "           4.0779e-02, -9.2092e-02,  9.5251e-01,  1.0651e-01, -9.8352e-01,\n",
       "           8.1500e-01, -2.5530e-01, -1.7528e-02,  9.4499e-02, -1.3070e-01,\n",
       "           7.5236e-01, -3.3303e-01,  4.6745e-01,  5.6456e-01, -7.9184e-01,\n",
       "          -5.1528e-01,  9.7030e-01, -6.5078e-01, -1.3315e-02, -1.2356e-01,\n",
       "          -1.6140e-01, -1.9592e-01,  9.0679e-01,  5.0304e-01,  2.8978e-01,\n",
       "          -7.9427e-01, -9.4486e-01,  5.4686e-01, -3.0045e-02, -6.4709e-01,\n",
       "           8.4360e-01,  7.0458e-02,  3.2529e-01, -2.5694e-01, -3.4180e-01,\n",
       "           6.0242e-01, -4.4963e-01, -5.9560e-02,  2.5652e-01, -1.6201e-01,\n",
       "           1.9459e-01,  4.6193e-01, -6.7766e-01, -1.6276e-01, -4.7854e-01,\n",
       "           3.6524e-01,  7.6033e-02, -2.0023e-02,  5.0411e-01,  1.1388e-01,\n",
       "           7.3077e-01,  5.4128e-01, -4.4955e-01, -1.5887e-01, -3.0344e-02,\n",
       "           5.3559e-01, -6.0842e-01,  7.6659e-01,  2.6770e-01, -6.3547e-01,\n",
       "          -9.4358e-01,  7.3117e-01,  5.4823e-01,  4.0641e-01, -3.4175e-01,\n",
       "          -7.6386e-01,  7.6256e-02,  4.6238e-01,  9.6913e-01,  6.0515e-01,\n",
       "           1.4729e-01,  3.0535e-01,  9.3565e-01,  5.7805e-01, -2.4722e-01,\n",
       "           4.6845e-02, -7.9401e-01, -6.8925e-01, -1.1449e-01, -5.9669e-01,\n",
       "           3.8669e-01,  2.1394e-01,  4.7335e-01, -8.8352e-01, -2.4208e-01,\n",
       "           8.6526e-01,  8.4671e-01, -5.1652e-02,  1.7291e-01,  1.1222e-01],\n",
       "         [-5.3716e-01, -9.8028e-01, -3.3078e-02, -7.9637e-01,  9.3271e-01,\n",
       "          -6.4012e-01,  2.4208e-01, -8.8646e-01,  4.3766e-01, -7.1430e-01,\n",
       "          -4.4036e-01,  6.8102e-01,  5.9713e-01,  5.6685e-01,  5.3112e-01,\n",
       "          -3.6177e-01, -7.5665e-01, -7.2774e-01,  3.2107e-01, -4.9841e-01,\n",
       "          -5.6335e-01, -3.4947e-02,  1.6577e-01,  9.9063e-01, -6.6646e-01,\n",
       "          -2.8039e-01, -4.1322e-01,  7.3165e-01, -8.1249e-01,  4.5499e-01,\n",
       "           3.6373e-01, -6.1807e-01,  9.5421e-01,  3.5192e-02, -4.9866e-01,\n",
       "          -5.4321e-01, -8.8548e-01,  9.4226e-01,  2.3880e-01,  3.0735e-01,\n",
       "           9.5779e-01, -4.0686e-01, -9.8638e-01,  6.3289e-02,  7.2621e-01,\n",
       "           6.1348e-01, -5.5952e-02,  9.5639e-02, -5.6115e-02,  6.7171e-01,\n",
       "           7.0353e-01, -9.9218e-01,  9.0462e-01,  8.5517e-01,  9.4931e-01,\n",
       "           5.5084e-01,  8.2644e-01,  2.3455e-01,  2.8799e-01, -4.9700e-01,\n",
       "           9.8232e-02, -4.3707e-01,  9.3382e-01,  6.4775e-01,  6.8255e-01,\n",
       "          -4.4261e-01, -9.3853e-01,  3.9502e-01,  4.1436e-01, -4.0217e-01,\n",
       "          -3.2867e-01,  2.6981e-01, -3.4292e-01, -9.1517e-01, -4.6053e-01,\n",
       "           7.7713e-01, -1.9259e-01,  5.1060e-01, -8.0132e-01,  6.1554e-01,\n",
       "           9.9445e-01,  3.3450e-02, -9.6336e-01, -5.7294e-01, -8.5675e-02,\n",
       "          -5.5751e-01, -2.7837e-02,  6.1976e-01, -8.1701e-01,  4.5887e-01,\n",
       "          -9.8259e-01,  2.1754e-01, -7.1949e-01, -7.9801e-01, -7.3620e-01,\n",
       "           3.3410e-01, -3.6268e-01, -5.6857e-02, -6.4453e-01,  8.2616e-01],\n",
       "         [-7.4324e-01, -3.7153e-01, -3.9084e-01,  6.9424e-01, -2.9321e-01,\n",
       "          -5.0767e-01, -8.6792e-01, -5.2473e-01, -8.4192e-01,  8.6688e-01,\n",
       "          -1.8623e-01,  9.6004e-01,  1.7194e-02,  5.6530e-01, -2.9294e-01,\n",
       "          -9.3113e-01, -7.7568e-01, -5.2866e-01, -6.3111e-01, -4.1443e-01,\n",
       "          -3.8074e-01,  5.1793e-01,  5.4974e-01, -4.8474e-01,  5.5344e-01,\n",
       "          -8.0463e-01,  9.2902e-01, -3.9690e-01,  9.9981e-01, -8.8171e-02,\n",
       "           9.7086e-01,  5.1007e-01, -4.4044e-01, -4.2920e-01,  7.6040e-01,\n",
       "           4.9674e-01, -9.5438e-01, -1.7780e-01, -6.7691e-01,  6.5247e-01,\n",
       "          -5.3853e-01,  9.1908e-01,  7.0199e-01,  3.3388e-01,  7.1704e-01,\n",
       "          -7.2398e-01,  9.9565e-01,  1.6686e-01,  3.0518e-01, -2.5504e-01,\n",
       "          -7.9948e-01, -1.8200e-01, -9.3036e-01,  6.9055e-01,  6.0098e-01,\n",
       "          -8.5126e-01,  5.1814e-02, -6.7144e-01, -6.5182e-03, -5.3591e-01,\n",
       "          -3.9929e-01,  4.3377e-01,  3.4497e-02,  7.5941e-01, -9.1199e-01,\n",
       "           7.1130e-01,  5.4772e-02, -4.5231e-01,  4.3515e-01,  6.1000e-01,\n",
       "          -9.6500e-01, -4.6211e-01, -6.5922e-01, -3.8650e-01,  8.0738e-01,\n",
       "          -7.2047e-01,  1.9804e-01,  7.0560e-01, -9.0571e-01, -7.6685e-01,\n",
       "           9.9485e-02, -6.2280e-01, -8.7532e-01,  7.6795e-01, -7.3454e-01,\n",
       "          -3.8307e-01, -1.2066e-02,  7.5101e-01,  8.9862e-01,  8.5688e-01,\n",
       "          -7.8661e-01, -2.9368e-01,  3.1617e-01, -5.7520e-01, -6.1666e-01,\n",
       "           4.3351e-01, -7.4218e-01,  5.1252e-01, -1.6578e-01,  1.2935e-01],\n",
       "         [ 3.4376e-01, -2.0688e-02,  9.2886e-01,  5.8107e-01,  7.4426e-01,\n",
       "          -3.1067e-01, -6.7392e-01,  4.9653e-03,  4.9147e-01,  9.4476e-01,\n",
       "          -2.1879e-01,  7.2944e-01,  1.1246e-01, -5.1268e-01,  7.4201e-01,\n",
       "           8.9404e-01, -7.7630e-01,  6.4465e-01, -2.4391e-01,  7.3475e-02,\n",
       "          -6.8100e-01,  6.2271e-01, -9.7683e-01, -6.6970e-02,  7.8934e-01,\n",
       "          -6.1973e-01, -5.9963e-01,  4.2661e-01,  1.6189e-01, -9.2168e-01,\n",
       "          -5.4234e-01, -2.2614e-01, -7.3724e-01,  8.9702e-01, -5.3716e-01,\n",
       "           9.0301e-01, -2.5226e-01,  3.2306e-01, -3.1248e-01, -9.2244e-01,\n",
       "          -3.1047e-01, -7.0218e-01, -4.6917e-01,  8.8725e-01, -1.2757e-01,\n",
       "           8.3672e-01,  8.6280e-01, -1.9833e-01,  2.2484e-02,  9.6742e-01,\n",
       "          -5.6460e-01,  6.0046e-01,  4.8008e-01,  4.5492e-01,  2.0395e-01,\n",
       "          -9.8457e-01, -5.2457e-01, -8.2486e-01, -4.6843e-01, -1.3568e-01,\n",
       "           8.8864e-01,  5.7165e-01,  3.2444e-02, -9.9785e-01, -9.9520e-02,\n",
       "          -7.6460e-01,  6.5985e-01,  6.1995e-01,  1.6864e-01, -8.2150e-01,\n",
       "          -1.4249e-01, -4.0626e-01,  7.2435e-01, -6.1991e-01,  4.4584e-01,\n",
       "           1.6598e-01,  3.4589e-01,  1.3810e-01, -5.3017e-01,  5.0524e-01,\n",
       "          -3.5230e-01, -6.7348e-01,  3.7552e-01,  1.4825e-01,  8.9532e-01,\n",
       "           5.7645e-01,  7.7151e-01,  8.8679e-01,  8.6299e-01, -9.6959e-02,\n",
       "          -6.5236e-01, -6.0612e-01, -8.1909e-01,  3.8767e-01,  6.8671e-01,\n",
       "          -1.2466e-02, -1.7952e-01, -7.4983e-01, -9.0645e-01,  7.4464e-01],\n",
       "         [-3.9590e-01,  3.8302e-01, -1.4173e-02, -8.5160e-01,  7.1255e-01,\n",
       "           5.8763e-01, -9.4919e-01,  9.7211e-01, -8.4606e-01,  4.8169e-01,\n",
       "           2.3588e-02, -3.6673e-01, -2.7931e-01, -8.8569e-01, -7.0827e-01,\n",
       "           3.6318e-01,  6.9758e-01, -3.8025e-01,  1.5352e-02, -6.3385e-02,\n",
       "           2.0046e-01, -8.8709e-01, -7.6983e-01,  4.3680e-01, -6.0075e-01,\n",
       "           9.3229e-01,  4.6760e-01,  4.2395e-01, -7.4088e-01,  8.1922e-01,\n",
       "          -9.7744e-01, -5.7858e-01, -1.1326e-01,  9.9605e-01, -6.2990e-01,\n",
       "          -2.6487e-01, -8.4035e-02,  8.8632e-01,  3.7866e-01,  4.0320e-01,\n",
       "           9.0358e-01, -4.0079e-01, -2.8677e-01,  8.1118e-01, -1.8360e-01,\n",
       "          -4.1767e-01,  2.0454e-01,  1.0842e-01,  1.8385e-01,  2.3712e-01,\n",
       "          -1.3042e-01,  5.9961e-01, -6.5027e-03, -3.0463e-01, -1.3479e-01,\n",
       "           7.6552e-01, -8.5035e-01,  1.1596e-01,  3.2800e-01,  3.9832e-01,\n",
       "          -9.1529e-01, -3.5699e-01,  8.6540e-01,  8.9543e-01, -2.7774e-01,\n",
       "          -4.4455e-02,  4.2342e-01,  1.2840e-01,  4.7368e-01,  9.2454e-01,\n",
       "          -5.5229e-01,  3.2267e-01,  1.2852e-01,  9.2682e-01,  8.6862e-01,\n",
       "          -2.0312e-01, -9.9921e-01, -8.7358e-01,  7.3212e-01, -9.5397e-01,\n",
       "          -6.6672e-01,  4.4849e-01,  3.2057e-01,  2.6488e-01,  3.5395e-01,\n",
       "           9.8020e-01, -4.7824e-01, -7.9491e-01, -4.4558e-01, -3.2139e-01,\n",
       "           6.8236e-01,  5.0300e-01, -3.3807e-01, -2.4502e-01, -7.4774e-02,\n",
       "           6.9799e-01,  4.0073e-01, -8.5842e-02,  4.4799e-01, -6.0910e-01]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sim = walk(polytope=hypercube_sim,\n",
    "        X0 = x0_sim,  \n",
    "        z=100, \n",
    "        T=1, \n",
    "        warm=0,\n",
    "        thinning=10000, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")\n",
    "X_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sim.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
