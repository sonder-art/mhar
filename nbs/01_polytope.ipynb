{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polytope Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp polytope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import  Union\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mhar import warningss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Polytope:\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float16,\n",
    "                device = None,\n",
    "                copy:bool=False,\n",
    "                requires_grad:bool=False\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        if device is None:\n",
    "            self.device = 'cpu' if A_in.get_device()==-1 else f'cuda:{A_in.get_device()}'\n",
    "        else:\n",
    "            self.device = device\n",
    "        self._assert_valid_device_(device=device)\n",
    "        \n",
    "        self._check_dtype_()\n",
    "        self._check_intra_constraint_dimensions_(A_in,b_in,'Inequality')\n",
    "        \n",
    "        self.copy = copy\n",
    "        if copy:\n",
    "            warnings.warn('The object will create a copy of the tensors, so memory usage will increase')\n",
    "        else:\n",
    "            warnings.warn('The object will not create a copy of the tensors, so modifications will be reflected in the object')\n",
    "\n",
    "        if requires_grad:\n",
    "            warnings.warn('The tensors will accumalate the gradient of the operations')\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        self.mI = A_in.shape[0]\n",
    "        self.n = A_in.shape[1]        \n",
    "        self.A_in = self._process_tensor_or_array_(A_in,'A_in')\n",
    "        self.b_in = self._process_tensor_or_array_(b_in,'b_in')\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "    def _check_intra_constraint_dimensions_(self, \n",
    "                        A:Union[torch.Tensor, np.ndarray], \n",
    "                        b:Union[torch.Tensor, np.ndarray], \n",
    "                        constraint:str=None):\n",
    "    \n",
    "        assert(A.shape[0] == b.shape[0]), f\"{constraint} has dimension mismatch: A has shape {A.shape} and b {b.shape}.\"\n",
    "    \n",
    "    \n",
    "    def _process_tensor_or_array_(self, \n",
    "                                 A:Union[torch.Tensor, np.ndarray], \n",
    "                                 restriction:str=None):\n",
    "        gc.collect()\n",
    "        dtype = self.dtype\n",
    "        if isinstance(A, torch.Tensor): # Torch Tensor\n",
    "            # A is already a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = A.clone()\n",
    "            else:\n",
    "                tensor_A = A\n",
    "            if dtype is not None and A.dtype != dtype:\n",
    "                tensor_A = tensor_A.to(dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        elif isinstance(A, np.ndarray): # Numpy array\n",
    "            # A is a NumPy array, convert it to a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = torch.tensor(A, dtype=dtype)\n",
    "            else:\n",
    "                tensor_A = torch.from_numpy(A, dtype=dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        else:\n",
    "            raise ValueError(f\"Input {restriction} must be a NumPy array or PyTorch tensor\")\n",
    "\n",
    "    def _check_dtype_(self):\n",
    "        valid_dtypes = (torch.float16, torch.float32, torch.float64)\n",
    "        assert self.dtype in valid_dtypes, f'{self.dtype} is not a valid PyTorch float data type. Valid dtypes: {valid_dtypes}'\n",
    "        if '16' in str(self.dtype): \n",
    "            long_message = f'The dtype {self.dtype} is typically used with GPU architectures. If you are using CPU, consider using 32 or 64-bit dtypes. \\\n",
    "Certain operations may be casted to 32 or 64 bits to enhance numerical stability.'\n",
    "\n",
    "            warnings.warn(long_message)\n",
    "            \n",
    "    def _assert_valid_device_(self, device):\n",
    "        \"\"\"\n",
    "        Asserts that the given device is valid in PyTorch.\n",
    "\n",
    "        Args:\n",
    "        device (str): The device to check, e.g., 'cuda:0', 'cpu'.\n",
    "        \"\"\"\n",
    "        # Check if the device is 'cpu' or 'cuda'. If 'cuda', check if it's available\n",
    "        if device == 'cpu' or (device.startswith('cuda') and torch.cuda.is_available()):\n",
    "            # If device is cuda, additionally check if the specified index is valid\n",
    "            if device.startswith('cuda'):\n",
    "                device_index = int(device.split(':')[1]) if ':' in device else 0\n",
    "                assert device_index < torch.cuda.device_count(), \"CUDA device index is out of range.\"\n",
    "        else:\n",
    "            raise AssertionError(f\"Device '{device}' is not valid. Please choose 'cpu' or a valid 'cuda' device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def send_to_device(self:Polytope, device:str=None):\n",
    "    self.A_in = self.A_in.to(device)\n",
    "    self.b_in = self.b_in.to(device)\n",
    "    self.device = device\n",
    "    \n",
    "@patch\n",
    "def cast_precision(self:Polytope, dtype=None):\n",
    "    if dtype is None:\n",
    "        dtype = self.dtype\n",
    "    self.A_in = self.A_in.to(dtype)\n",
    "    self.b_in = self.b_in.to(dtype)\n",
    "    self.dtype = dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:Polytope):\n",
    "    string = f'Numeric Precision (dtype) {self.dtype}\\n'\n",
    "    string = string + f'Device: {self.device}\\n'\n",
    "    string = string + f'A_in: {self.A_in.shape} \\n'\n",
    "    string = string + f'b_in: {self.b_in.shape}'\n",
    "    return string\n",
    "\n",
    "@patch\n",
    "def __repr__(self:Polytope):\n",
    "    return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NFDPolytope(Polytope):\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                A_eq:Union[torch.Tensor, np.ndarray], \n",
    "                b_eq:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float32, # Default dtype for Non-Fully-Dimensioonal Polytopes is 32 bits\n",
    "                device = None,\n",
    "                copy:bool=False,\n",
    "                requires_grad:bool=False\n",
    "                 ) -> None:\n",
    "        \n",
    "        self._check_intra_constraint_dimensions_(A_in,b_in,'Inequality')\n",
    "        self._check_intra_constraint_dimensions_(A_eq,b_eq,'Equality')\n",
    "        self._check_inter_constraint_dimensions_(A_in, A_eq)\n",
    "        \n",
    "        if '16' in str(dtype):\n",
    "            warnings.warn('Float16 precision was selected, since there are some equality restrictions a Projection matrix must be computed. Be sure to evaluate the numerical stability of the algorithm. We suggest using float32 or bigger.')\n",
    "\n",
    "        super().__init__(A_in, b_in, dtype, device,copy, requires_grad)\n",
    "        self.mE = A_eq.shape[0]\n",
    "        self.A_eq = self._process_tensor_or_array_(A_eq,'A_eq')\n",
    "        self.b_eq = self._process_tensor_or_array_(b_eq,'b_eq')\n",
    "        self.projection_matrix = None\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def _check_inter_constraint_dimensions_(self,A_in,A_eq):\n",
    "        \n",
    "        assert(A_in.shape[1] == A_eq.shape[1]), f\"Constraints have dimension mismatch: A_in has shape {A_in.shape} and  A_eq {A_eq.shape}.\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def send_to_device(self:NFDPolytope, device:str):\n",
    "    Polytope.send_to_device(self,device)\n",
    "    self.device = None\n",
    "    self.A_eq = self.A_eq.to(device)\n",
    "    self.b_eq = self.b_eq.to(device)\n",
    "    if self.projection_matrix is not None:\n",
    "        self.projection_matrix = self.projection_matrix.to(device)\n",
    "    self.device = device\n",
    "\n",
    "@patch\n",
    "def cast_precision(self:NFDPolytope, dtype=None):\n",
    "    Polytope.cast_precision(self, dtype)\n",
    "    self.dtype = None\n",
    "    self.A_eq = self.A_eq.to(dtype)\n",
    "    self.b_eq = self.b_eq.to(dtype)\n",
    "    if self.projection_matrix is not None:\n",
    "        self.projection_matrix = self.projection_matrix.to(dtype)\n",
    "    self.dtype = dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def compute_projection_matrix(self:NFDPolytope, device:str, solver_precision=torch.float64):\n",
    "\n",
    "    ## Set Device\n",
    "    original_device = self.device\n",
    "    self.send_to_device(device)\n",
    "    \n",
    "    ## Set Precision\n",
    "    if '64' in str(solver_precision):\n",
    "        solver_precision_tensor = torch.float64\n",
    "    elif '32' in str(solver_precision):\n",
    "        solver_precision_tensor = torch.float32\n",
    "    elif '16' in str(solver_precision):\n",
    "        solver_precision_tensor = torch.float16\n",
    "    original_precision = self.dtype\n",
    "    error_precision = self.dtype\n",
    "        \n",
    "    if ('cuda' not in device) & ('16' in str(original_precision)):\n",
    "        warnings.warn('Float16 precision was chosen for the polytope, but the \"device=cpu\" option is selected. Tensors will be temporarily cast to float32 for computing the Projection Matrix and stability evaluation. If you wish to use float16 precision, please select \"device=cuda\".')\n",
    "        solver_precision_tensor = torch.float32\n",
    "        error_precision = torch.float32\n",
    "    self.cast_precision(solver_precision_tensor)\n",
    "    \n",
    "    # Compute (A A')^(-1)\n",
    "    A_eq_t = torch.transpose(self.A_eq, 0, 1)\n",
    "    A_eq_mm_A_eq_t = torch.matmul(self.A_eq, A_eq_t)\n",
    "    \n",
    "    # Compute (A A')^(-1)A\n",
    "    la = torch.linalg.solve(A_eq_mm_A_eq_t,self.A_eq)\n",
    "\n",
    "    # Check numerical stability of (A A')^(-1) (AA') - I\n",
    "    \n",
    "    est = torch.mm(la.to(error_precision), A_eq_t.to(error_precision))\n",
    "    est = torch.max(torch.abs(est - torch.eye(est.shape[0], device=device).to(error_precision)))\n",
    "    print(f\"Max non zero error for term (A A')^(-1)A at precision {error_precision}: \", est)\n",
    "    del est\n",
    "\n",
    "    # Compute I - A'(A A')^(-1)A\n",
    "    projection_matrix = torch.matmul(A_eq_t,la)\n",
    "    projection_matrix = torch.eye(projection_matrix.shape[0], device=device) - projection_matrix\n",
    "\n",
    "    # Free Memory\n",
    "    del A_eq_mm_A_eq_t\n",
    "    del A_eq_t\n",
    "    del la\n",
    "    gc.collect()\n",
    "    self.projection_matrix =  projection_matrix\n",
    "        \n",
    "    self.send_to_device(original_device)    \n",
    "    self.cast_precision(original_precision)\n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def project_h(self:NFDPolytope, x):\n",
    "    assert(self.projection_matrix is not None), f'Projection Matrix has not been computed.'\n",
    "        \n",
    "    x_projected = torch.matmul(self.projection_matrix,x)\n",
    "    return x_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:NFDPolytope):\n",
    "    string = Polytope.__str__(self)\n",
    "    string = string + f'\\nA_eq: {self.A_eq.shape} \\nb_eq: {self.b_eq.shape}'\n",
    "    if self.projection_matrix is not None:\n",
    "        string = string + f'\\nProjection Matrix: {self.projection_matrix.shape}'\n",
    "    return string\n",
    "\n",
    "@patch\n",
    "def __repr__(self:NFDPolytope):\n",
    "    return self.__str__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
