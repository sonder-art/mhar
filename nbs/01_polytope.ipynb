{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polytope Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp polytope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import  Union\n",
    "import gc\n",
    "\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Polytope:\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float16,\n",
    "                copy:bool=False,\n",
    "                requires_grad:bool=False\n",
    "                 ) -> None:\n",
    "        \n",
    "        self._check_intra_constraint_dimension_(A_in,b_in,'Inequality')\n",
    "        \n",
    "        self.copy = copy\n",
    "        if requires_grad:\n",
    "            print('The tensors will accumalate the gradient of the operations')\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        self.mI = A_in.shape[0]\n",
    "        self.n = A_in.shape[1]        \n",
    "        self.A_in = self._process_tensor_or_array(A_in,'A_in')\n",
    "        self.b_in = self._process_tensor_or_array(b_in,'b_in')\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "    def _check_intra_constraint_dimensions_(self, \n",
    "                        A:Union[torch.Tensor, np.ndarray], \n",
    "                        b:Union[torch.Tensor, np.ndarray], \n",
    "                        constraint:str=None):\n",
    "    \n",
    "        assert(A.shape[0] == b.shape[0]), f\"{constraint} has dimension mismatch: A has shape {A.shape} and b {b.shape}.\"\n",
    "    \n",
    "    \n",
    "    def _process_tensor_or_array_(self, \n",
    "                                 A:Union[torch.Tensor, np.ndarray], \n",
    "                                 restriction:str=None):\n",
    "        gc.collect()\n",
    "        dtype = self.dtype\n",
    "        if isinstance(A, torch.Tensor): # Torch Tensor\n",
    "            # A is already a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = A.clone()\n",
    "            else:\n",
    "                tensor_A = A\n",
    "            if dtype is not None and A.dtype != dtype:\n",
    "                tensor_A = tensor_A.to(dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        elif isinstance(A, np.ndarray): # Numpy array\n",
    "            # A is a NumPy array, convert it to a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = torch.tensor(A, dtype=dtype)\n",
    "            else:\n",
    "                tensor_A = torch.from_numpy(A, dtype=dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        else:\n",
    "            raise ValueError(f\"Input {restriction} must be a NumPy array or PyTorch tensor\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:Polytope):\n",
    "    string = f'Numeric Precision (dtype) {self.dtype}'\n",
    "    string = string + f'A_in: {self.A_in.shape} \\n'\n",
    "    string = string + f'b_in: {self.b_in.shape}'\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NFDPolytope(Polytope):\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                A_eq:Union[torch.Tensor, np.ndarray], \n",
    "                b_eq:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float32 # Default dtype for Non-Fully-Dimensioonal Polytopes is 32 bits\n",
    "                 ) -> None:\n",
    "        \n",
    "        self._check_intra_constraint_dimension_(A_in,b_in,'Inequality')\n",
    "        self._check_intra_constraint_dimension_(A_eq,b_eq,'Equality')\n",
    "        self._check_inter_constraint_dimensions_(A_in, A_eq)\n",
    "\n",
    "        super.__init__(self, A_in, b_in, dtype)\n",
    "        self.mE = A_eq.shape[0]\n",
    "        self.A_eq = self._process_tensor_or_array(A_eq,'A_eq')\n",
    "        self.b_eq = self._process_tensor_or_array(b_eq,'b_eq')\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def _check_inter_constraint_dimensions_(self,A_in,A_eq):\n",
    "        \n",
    "        assert(A_in.shape[0] == A_eq.shape[0]), f\"Constraints have dimension mismatch: A_in has shape {A_in.shape} and  A_eq {A_eq.shape}.\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:NFDPolytope):\n",
    "    string = super.__str__()\n",
    "    string = string + f'A_eq: {self.A_eq.shape} \\n b_eq: {self.b_eq.shape}'\n",
    "    return string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
