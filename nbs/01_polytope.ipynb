{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polytope Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp polytope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import  Union\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from mhar import warningss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Polytope:\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float16,\n",
    "                copy:bool=False,\n",
    "                requires_grad:bool=False\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        self.device = None\n",
    "        \n",
    "        self._check_dtype_()\n",
    "        self._check_intra_constraint_dimensions_(A_in,b_in,'Inequality')\n",
    "        \n",
    "        self.copy = copy\n",
    "        if copy:\n",
    "            warnings.warn('The object will create a copy of the tensors, so memory usage will increase')\n",
    "        else:\n",
    "            warnings.warn('The object will not create a copy of the tensors, so modifications will be reflected in the object')\n",
    "\n",
    "        if requires_grad:\n",
    "            warnings.warn('The tensors will accumalate the gradient of the operations')\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        self.mI = A_in.shape[0]\n",
    "        self.n = A_in.shape[1]        \n",
    "        self.A_in = self._process_tensor_or_array_(A_in,'A_in')\n",
    "        self.b_in = self._process_tensor_or_array_(b_in,'b_in')\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "    def _check_intra_constraint_dimensions_(self, \n",
    "                        A:Union[torch.Tensor, np.ndarray], \n",
    "                        b:Union[torch.Tensor, np.ndarray], \n",
    "                        constraint:str=None):\n",
    "    \n",
    "        assert(A.shape[0] == b.shape[0]), f\"{constraint} has dimension mismatch: A has shape {A.shape} and b {b.shape}.\"\n",
    "    \n",
    "    \n",
    "    def _process_tensor_or_array_(self, \n",
    "                                 A:Union[torch.Tensor, np.ndarray], \n",
    "                                 restriction:str=None):\n",
    "        gc.collect()\n",
    "        dtype = self.dtype\n",
    "        if isinstance(A, torch.Tensor): # Torch Tensor\n",
    "            # A is already a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = A.clone()\n",
    "            else:\n",
    "                tensor_A = A\n",
    "            if dtype is not None and A.dtype != dtype:\n",
    "                tensor_A = tensor_A.to(dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        elif isinstance(A, np.ndarray): # Numpy array\n",
    "            # A is a NumPy array, convert it to a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = torch.tensor(A, dtype=dtype)\n",
    "            else:\n",
    "                tensor_A = torch.from_numpy(A, dtype=dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        else:\n",
    "            raise ValueError(f\"Input {restriction} must be a NumPy array or PyTorch tensor\")\n",
    "\n",
    "    def _check_dtype_(self):\n",
    "        valid_dtypes = (torch.float16, torch.float32, torch.float64)\n",
    "        assert self.dtype in valid_dtypes, f'{self.dtype} is not a valid PyTorch float data type.'\n",
    "        if '16' in str(self.dtype): \n",
    "            long_message = f'The dtype {self.dtype} is typically used with GPU architectures. If you are using CPU, consider using 32 or 64-bit dtypes. \\\n",
    "Certain operations may be casted to 32 or 64 bits to enhance numerical stability.'\n",
    "\n",
    "            warnings.warn(long_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def send_to_device(self:Polytope, device:str=None):\n",
    "    self.A_in = self.A_in.to(device)\n",
    "    self.b_in = self.b_in.to(device)\n",
    "    self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:Polytope):\n",
    "    string = f'Numeric Precision (dtype) {self.dtype}\\n'\n",
    "    string = string + f'Device: {self.device}\\n'\n",
    "    string = string + f'A_in: {self.A_in.shape} \\n'\n",
    "    string = string + f'b_in: {self.b_in.shape}'\n",
    "    return string\n",
    "\n",
    "@patch\n",
    "def __repr__(self:Polytope):\n",
    "    return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NFDPolytope(Polytope):\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                A_eq:Union[torch.Tensor, np.ndarray], \n",
    "                b_eq:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float32, # Default dtype for Non-Fully-Dimensioonal Polytopes is 32 bits\n",
    "                copy:bool=False,\n",
    "                requires_grad:bool=False\n",
    "                 ) -> None:\n",
    "        \n",
    "        self._check_intra_constraint_dimensions_(A_in,b_in,'Inequality')\n",
    "        self._check_intra_constraint_dimensions_(A_eq,b_eq,'Equality')\n",
    "        self._check_inter_constraint_dimensions_(A_in, A_eq)\n",
    "\n",
    "        super().__init__(A_in, b_in, dtype, copy, requires_grad)\n",
    "        self.mE = A_eq.shape[0]\n",
    "        self.A_eq = self._process_tensor_or_array_(A_eq,'A_eq')\n",
    "        self.b_eq = self._process_tensor_or_array_(b_eq,'b_eq')\n",
    "        self.projection_matrix = None\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def _check_inter_constraint_dimensions_(self,A_in,A_eq):\n",
    "        \n",
    "        assert(A_in.shape[1] == A_eq.shape[1]), f\"Constraints have dimension mismatch: A_in has shape {A_in.shape} and  A_eq {A_eq.shape}.\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def send_to_device(self:NFDPolytope, device:str):\n",
    "    Polytope.send_to_device(self,device)\n",
    "    self.device = None\n",
    "    self.A_eq = self.A_eq.to(device)\n",
    "    self.b_eq = self.b_eq.to(device)\n",
    "    if self.projection_matrix is not None:\n",
    "        self.projection_matrix = self.projection_matrix.to(device)\n",
    "    self.device = device\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def compute_projection_matrix(self:NFDPolytope, device:str, max_precision:bool=True):\n",
    "    if max_precision:\n",
    "        precision = torch.float64\n",
    "    else:\n",
    "        precision = self.dtype\n",
    "        \n",
    "    if ('cuda' not in device) & ('16' in str(precision)):\n",
    "        warnings.warn('Float16 precision was chosen for the polytope, but the \"device=cpu\" option is selected. Tensors will be temporarily cast to float32 for stability evaluation. If you wish to use float16 precision, please select \"device=cuda\".')\n",
    "        precision = torch.float32\n",
    "\n",
    "        \n",
    "    self.send_to_device(device)\n",
    "    \n",
    "    # Compute (A A')^(-1)\n",
    "    A_eq_t = torch.transpose(self.A_eq.to(precision), 0, 1)\n",
    "    A_eq_mm_A_eq_t = torch.matmul(self.A_eq.to(precision), A_eq_t.to(precision))\n",
    "    #ae_inv = torch.inverse(ae_aux)\n",
    "    \n",
    "    # Compute (A A')^(-1)A\n",
    "    la = torch.linalg.solve(A_eq_mm_A_eq_t.to(precision),self.A_eq.to(precision)).to(precision)\n",
    "\n",
    "    # Check numerical stability of (A A')^(-1) (AA') - I\n",
    "    est = torch.mm(la, A_eq_t)\n",
    "    est = torch.max(torch.abs(est - torch.eye(est.shape[0], device=device)))\n",
    "    print(\"Max non zero error for term (A A')^(-1)A: \", est)\n",
    "    del est\n",
    "\n",
    "    # Compute I - A'(A A')^(-1)A\n",
    "    #la = torch.matmul(aet, ae_inv)\n",
    "    projection_matrix = torch.matmul(A_eq_t.to(precision),la.to(precision)).to(self.dtype)\n",
    "    projection_matrix = torch.eye(projection_matrix.shape[0], device=device).to(self.dtype) - projection_matrix\n",
    "\n",
    "    # Free Memory\n",
    "    del A_eq_mm_A_eq_t\n",
    "    del A_eq_t\n",
    "    #del ae_inv\n",
    "    del la\n",
    "    gc.collect()\n",
    "    self.projection_matrix =  projection_matrix\n",
    "    if self.device is None:\n",
    "        self.send_to_device(device='cpu')\n",
    "    else:\n",
    "        self.send_to_device(device=self.device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:NFDPolytope):\n",
    "    string = Polytope.__str__(self)\n",
    "    string = string + f'\\nA_eq: {self.A_eq.shape} \\nb_eq: {self.b_eq.shape}'\n",
    "    if self.projection_matrix is not None:\n",
    "        string = string + f'\\nProjection Matrix: {self.projection_matrix.shape}'\n",
    "    return string\n",
    "\n",
    "@patch\n",
    "def __repr__(self:NFDPolytope):\n",
    "    return self.__str__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
