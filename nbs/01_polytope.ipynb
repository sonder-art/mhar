{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polytope Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp polytope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import  Union\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Polytope:\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float16,\n",
    "                copy:bool=False,\n",
    "                requires_grad:bool=False\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        self._check_dtype_()\n",
    "        self._check_intra_constraint_dimension_(A_in,b_in,'Inequality')\n",
    "        \n",
    "        self.copy = copy\n",
    "        if copy:\n",
    "            warnings.warn('The object will create a copy of the tensors, so memory usage will increase')\n",
    "        else:\n",
    "            warnings.warn('The object will not create a copy of the tensors, so modifications    will be reflected ')\n",
    "            \n",
    "\n",
    "        if requires_grad:\n",
    "            warnings.warn('The tensors will accumalate the gradient of the operations')\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        self.mI = A_in.shape[0]\n",
    "        self.n = A_in.shape[1]        \n",
    "        self.A_in = self._process_tensor_or_array(A_in,'A_in')\n",
    "        self.b_in = self._process_tensor_or_array(b_in,'b_in')\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "    def _check_intra_constraint_dimensions_(self, \n",
    "                        A:Union[torch.Tensor, np.ndarray], \n",
    "                        b:Union[torch.Tensor, np.ndarray], \n",
    "                        constraint:str=None):\n",
    "    \n",
    "        assert(A.shape[0] == b.shape[0]), f\"{constraint} has dimension mismatch: A has shape {A.shape} and b {b.shape}.\"\n",
    "    \n",
    "    \n",
    "    def _process_tensor_or_array_(self, \n",
    "                                 A:Union[torch.Tensor, np.ndarray], \n",
    "                                 restriction:str=None):\n",
    "        gc.collect()\n",
    "        dtype = self.dtype\n",
    "        if isinstance(A, torch.Tensor): # Torch Tensor\n",
    "            # A is already a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = A.clone()\n",
    "            else:\n",
    "                tensor_A = A\n",
    "            if dtype is not None and A.dtype != dtype:\n",
    "                tensor_A = tensor_A.to(dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        elif isinstance(A, np.ndarray): # Numpy array\n",
    "            # A is a NumPy array, convert it to a PyTorch tensor\n",
    "            if self.copy:\n",
    "                tensor_A = torch.tensor(A, dtype=dtype)\n",
    "            else:\n",
    "                tensor_A = torch.from_numpy(A, dtype=dtype)\n",
    "            return tensor_A.requires_grad_(self.requires_grad)\n",
    "        else:\n",
    "            raise ValueError(f\"Input {restriction} must be a NumPy array or PyTorch tensor\")\n",
    "\n",
    "    def _check_dtype_(self):\n",
    "        assert(isinstance(self.dtype, torch.float16) or isinstance(self.dtype, torch.float32) or isinstance(self.dtype, torch.float64)), f'{self.dtype} is not a PyTorch float data type.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def send_to_device(self:Polytope, device:str=None):\n",
    "    self.A_in = self.A_in.to(device)\n",
    "    self.b_in = self.A_in.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:Polytope):\n",
    "    string = f'Numeric Precision (dtype) {self.dtype}'\n",
    "    string = string + f'A_in: {self.A_in.shape} \\n'\n",
    "    string = string + f'b_in: {self.b_in.shape}'\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NFDPolytope(Polytope):\n",
    "    \n",
    "    def __init__(self, \n",
    "                A_in:Union[torch.Tensor, np.ndarray], \n",
    "                b_in:Union[torch.Tensor, np.ndarray], \n",
    "                A_eq:Union[torch.Tensor, np.ndarray], \n",
    "                b_eq:Union[torch.Tensor, np.ndarray], \n",
    "                dtype=torch.float32 # Default dtype for Non-Fully-Dimensioonal Polytopes is 32 bits\n",
    "                 ) -> None:\n",
    "        \n",
    "        self._check_intra_constraint_dimension_(A_in,b_in,'Inequality')\n",
    "        self._check_intra_constraint_dimension_(A_eq,b_eq,'Equality')\n",
    "        self._check_inter_constraint_dimensions_(A_in, A_eq)\n",
    "\n",
    "        super.__init__(self, A_in, b_in, dtype)\n",
    "        self.mE = A_eq.shape[0]\n",
    "        self.A_eq = self._process_tensor_or_array(A_eq,'A_eq')\n",
    "        self.b_eq = self._process_tensor_or_array(b_eq,'b_eq')\n",
    "        self.projection_matrix = None\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def _check_inter_constraint_dimensions_(self,A_in,A_eq):\n",
    "        \n",
    "        assert(A_in.shape[0] == A_eq.shape[0]), f\"Constraints have dimension mismatch: A_in has shape {A_in.shape} and  A_eq {A_eq.shape}.\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def send_to_device(self:NFDPolytope, device:str):\n",
    "    super._send_to_device_(device)\n",
    "    self.A_eq = self.A_eq.to(device)\n",
    "    self.b_eq = self.b_eq.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def compute_projection_matrix(self:NFDPolytope, device:str):\n",
    "    \n",
    "    self._send_to_device_(device)\n",
    "    # Compute (A A')^(-1)\n",
    "    aet = torch.transpose(self.A_eq, 0, 1)\n",
    "    ae_aux = torch.matmul(self.A_eq, aet)\n",
    "    ae_inv = torch.inverse(ae_aux)\n",
    "\n",
    "    # Check numerical stability of (A A')^(-1)\n",
    "    est = torch.mm(ae_inv, ae_aux)\n",
    "    est = torch.max(torch.abs(est - torch.eye(est.shape[0], device=device)))\n",
    "    print('Max non zero error: ', est)\n",
    "    del est\n",
    "\n",
    "    # Compute I - A'(A A')^(-1)A\n",
    "    la = torch.matmul(aet, ae_inv)\n",
    "    projection_matrix = torch.matmul(la, self.A_eq)\n",
    "    projection_matrix = torch.eye(projection_matrix.shape[0], device=device) - projection_matrix\n",
    "\n",
    "    # Free Memory\n",
    "    del ae_aux\n",
    "    del ae_inv\n",
    "    del la\n",
    "    gc.collect()\n",
    "    self.send_to_device(device='cpu')\n",
    "\n",
    "    self.projection_matrix =  projection_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self:NFDPolytope):\n",
    "    string = super.__str__()\n",
    "    string = string + f'A_eq: {self.A_eq.shape} \\n b_eq: {self.b_eq.shape}'\n",
    "    return string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
