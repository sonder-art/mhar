{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Inner Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inner_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from typing import Union\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from mhar.polytope import Polytope, NFDPolytope\n",
    "from mhar import warningss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "\n",
    "def ChebyshevCenter(\n",
    "                    polytope:Union[Polytope, NFDPolytope], \n",
    "                    lb=None, # Lowerbound (lb <= x ), if unknown leave it as None \n",
    "                    ub=None, # Upperbound ( x <= up), if unknown leave it as None \n",
    "                    tolerance=1e-4, # Tolerance for equality restrictions (A_eqx = b_eq)\n",
    "                    device='cpu',\n",
    "                    solver_precision=np.float64\n",
    "                    ): \n",
    "    ## Set Device\n",
    "    polytope_device = polytope.device\n",
    "    polytope.send_to_device('cpu')\n",
    "    \n",
    "    ## Set Precision\n",
    "    if '64' in str(solver_precision):\n",
    "        solver_precision_tensor = torch.float64\n",
    "    elif '32' in str(solver_precision):\n",
    "        solver_precision_tensor = torch.float32\n",
    "    elif '16' in str(solver_precision):\n",
    "        solver_precision_tensor = torch.float16\n",
    "    original_precision = polytope.dtype\n",
    "    polytope.cast_precision(solver_precision_tensor)\n",
    "    \n",
    "                  \n",
    "    A_in = polytope.A_in.numpy()\n",
    "    b_in = polytope.b_in.numpy()\n",
    "    A_in_norm = np.sqrt(np.sum(A_in**2, axis=-1))[np.newaxis,:]\n",
    "    # Create new restriction matrices\n",
    "    A_in_norm = np.concatenate((A_in, A_in_norm.transpose()), axis=1)\n",
    "    c = np.concatenate((np.zeros(A_in.shape[1]), [-1.]))\n",
    "    del A_in \n",
    "    gc.collect()\n",
    "    \n",
    "    ## Inequality\n",
    "    if isinstance(polytope,NFDPolytope):\n",
    "        mE = polytope.mE\n",
    "        A_eq = polytope.A_eq.numpy()\n",
    "        # The equality restrictions have zero norm. Is transposed to keep order\n",
    "        A_eq_norm = np.zeros((1, A_eq.shape[0]))\n",
    "        # Create new restriction matrices\n",
    "        A_eq_norm = np.concatenate((A_eq, A_eq_norm.transpose()), axis=1)\n",
    "        del A_eq\n",
    "        gc.collect()\n",
    "        b_eq = polytope.b_eq.numpy()\n",
    "    else:\n",
    "        mE=0\n",
    "        A_eq_norm = None\n",
    "        b_eq = None\n",
    "        \n",
    "    r = linprog(c=c,\n",
    "        A_ub=A_in_norm,\n",
    "        b_ub=b_in,\n",
    "        A_eq=A_eq_norm,\n",
    "        b_eq=b_eq,\n",
    "        bounds = (lb, ub),\n",
    "        method= 'highs')\n",
    "\n",
    "    status = {0:'Optimization proceeding nominally.',\n",
    "                1: 'Iteration limit reached.',\n",
    "                2: 'Problem appears to be infeasible.',\n",
    "                3: ' Problem appears to be unbounded.',\n",
    "                4: ' Numerical difficulties encountered.',\n",
    "            }[r.status]\n",
    "    \n",
    "    print('\\nSimplex Status for the Chebyshev Center\\n', status)\n",
    "    \n",
    "    if ('cuda' not in device) & ('16' in str(original_precision)):\n",
    "        warnings.warn('Float16 precision was chosen for the polytope, but the \"device=cpu\" option is selected. Tensors will be temporarily cast to float32 for stability evaluation. If you wish to use float16 precision, please select \"device=cuda\".')\n",
    "        dtype = torch.float32\n",
    "    else:\n",
    "        dtype = polytope.dtype\n",
    "    \n",
    "    x0 = torch.tensor(np.array(r.x[:-1], ndmin=2).transpose(), dtype=dtype).to(device)\n",
    "    x0_in = torch.matmul(polytope.A_in.to(device).to(dtype), x0.to(device).to(dtype))\n",
    "    b_in = torch.from_numpy(b_in).to(device).to(dtype)\n",
    "    \n",
    "    \n",
    "    assert(torch.all(x0_in <= b_in )), f'Point {x0} does not satisfy A_inx <= b_in restrictions, it may have numerical inestability'\n",
    "    if mE>0:\n",
    "        x0_eq = torch.matmul(polytope.A_eq.to(device).to(dtype), x0.to(device).to(dtype))\n",
    "        b_eq = torch.from_numpy(b_eq).to(device).to(dtype)\n",
    "        assert(torch.all(torch.abs(x0_eq - b_eq) <= tolerance)), f'Point {x0} does not satisfy A_eqx = b_eq restrictions with tolerance {tolerance}, it may have numerical inestability. Try improving precision in the polytope and solver.'\n",
    "    \n",
    "    polytope.send_to_device(polytope_device)    \n",
    "    polytope.cast_precision(original_precision)\n",
    "              \n",
    "    \n",
    "    return x0.to(polytope_device).to(original_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:92: UserWarning:\n",
      "  The dtype torch.float16 is typically used with GPU architectures. If you are using CPU, consider\n",
      "  using 32 or 64-bit dtypes. Certain operations may be casted to 32 or 64 bits to enhance numerical\n",
      "  stability.\n",
      "\n",
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:40: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n",
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.],\n",
       "        [-0.],\n",
       "        [-0.]], dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mhar.polytope_examples import Hypercube\n",
    "import numpy as np\n",
    "\n",
    "hypercube = Hypercube(3,\n",
    "                      dtype=torch.float16\n",
    "                      )\n",
    "x0 = ChebyshevCenter(polytope=hypercube, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-4,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float64)\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:140: UserWarning:\n",
      "  Float16 precision was selected, since there are some equality restrictions a Projection matrix\n",
      "  must be computed. Be sure to evaluate the numerical stability of the algorithm.\n",
      "\n",
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:92: UserWarning:\n",
      "  The dtype torch.float16 is typically used with GPU architectures. If you are using CPU, consider\n",
      "  using 32 or 64-bit dtypes. Certain operations may be casted to 32 or 64 bits to enhance numerical\n",
      "  stability.\n",
      "\n",
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:40: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n",
      "Max non zero error for term (A A')^(-1)A at precision torch.float16:  tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100],\n",
       "        [0.0100]], dtype=torch.float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mhar.polytope_examples import Simplex\n",
    "import numpy as np\n",
    "\n",
    "simplex = Simplex(\n",
    "    n=100,\n",
    "    dtype=torch.float16,\n",
    "    copy=False,\n",
    "    requires_grad=False\n",
    ")\n",
    "simplex.compute_projection_matrix(device='cuda')\n",
    "\n",
    "x0 = ChebyshevCenter(polytope=simplex, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-10,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float64)\n",
    "x0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
