{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHAR walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mhar.polytope import Polytope, NFDPolytope\n",
    "from mhar.utils import print_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sample_inner_points(X0, z):\n",
    "    n, k = X0.shape\n",
    "    if z > k:\n",
    "        # Pad by repeating vectors from X to fill z columns\n",
    "        num_repeats = z // k\n",
    "        remainder = z % k\n",
    "        I = torch.cat([X0] * num_repeats + [X0[:, :remainder]], dim=1)\n",
    "    elif z < k:\n",
    "        # Take the first z columns from X\n",
    "        I = X0[:, :z]\n",
    "    else:\n",
    "        # z == k, I equals X\n",
    "        I = X0.clone()\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_h(n,z,generator,dtype,device):\n",
    "    \"\"\"\n",
    "    Creates a Tensor (z x n x 1) where each entry ~ N(0,1). Automatically detects the\n",
    "    precision 64 bits or 32 bits.\n",
    "    -------------\n",
    "    :param n:   int\n",
    "                Dimension of the Space where the Polytope Lives\n",
    "    :param z:   int\n",
    "                Padding Parameter\n",
    "    :param generator:\n",
    "    :param device:  String, default = cpu\n",
    "                    Hardware used to make the computations and allocate the result.\n",
    "                    If equal to cpu then the CPUs are used for computing the inverse.\n",
    "                    If equal to cuda then the a GPU is used for computing the inverse.\n",
    "    -------------\n",
    "    :return:    Torch Tensor\n",
    "                Tensor (z x n x 1) where each entry ~ N(0,1)Contains a tensor\n",
    "\n",
    "    \"\"\"\n",
    "    if '64' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            h = torch.cuda.DoubleTensor(n, z).normal_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            h = torch.DoubleTensor(n, z).normal_(generator=generator)\n",
    "    elif '32' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            h = torch.cuda.FloatTensor(n, z).normal_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            h = torch.FloatTensor(n, z).normal_(generator=generator)\n",
    "    elif '16' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            h = torch.cuda.HalfTensor(n, z).normal_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            h = torch.HalfTensor(n, z).normal_(generator=generator)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def draw_uniform(z, generator,dtype,device='cpu'):\n",
    "    \"\"\"\n",
    "    Creates a tensor (z x 1) where each entry ~ U(0,1). Automatically detects the\n",
    "    precision 64 bits or 32 bits.\n",
    "    -------------\n",
    "    :param n:   int\n",
    "                Dimension of the Space where the Polytope Lives\n",
    "    :param z:   int\n",
    "                Padding Parameter\n",
    "    :param generator:\n",
    "    :param device:  String, default = cpu\n",
    "                    Hardware used to make the computations and allocate the result.\n",
    "                    If equal to cpu then the CPUs are used for computing the inverse.\n",
    "                    If equal to cuda then the a GPU is used for computing the inverse.\n",
    "    -------------\n",
    "    :return:    Torch Tensor\n",
    "                Tensor (z x 1) where each entry ~ U(0,1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if '64' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            u = torch.cuda.DoubleTensor(1, z).uniform_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            u = torch.DoubleTensor(1,z).uniform_(generator=generator)\n",
    "    elif '32' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            u = torch.cuda.FloatTensor(1, z).uniform_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            u = torch.FloatTensor(1, z).uniform_(generator=generator)\n",
    "    elif '16' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            u = torch.cuda.HalfTensor(1, z).uniform_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            u = torch.HalfTensor(1, z).uniform_(generator=generator)\n",
    "\n",
    "    return u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "\n",
    "def update_progress_bars(burned, thinning, t, T, new_t: bool = False):\n",
    "    # Generate progress bar string for burned samples\n",
    "    if thinning !=0:\n",
    "        progress_burned = print_progress(iteration=burned, prefix='% of burned samples', total=thinning, length=30)\n",
    "\n",
    "    # Clear the current line\n",
    "    sys.stdout.write('\\033[K')\n",
    "\n",
    "    if new_t:\n",
    "        # Generate progress bar string for iid samples\n",
    "        progress_iid = print_progress(iteration=t, prefix='% of iid samples', total=T, length=30)\n",
    "        # Print both progress bars\n",
    "        if thinning !=0:\n",
    "            print(progress_burned, end='\\n')\n",
    "        print(progress_iid, end='\\n')\n",
    "        # Move cursor up two lines for the next update\n",
    "        #sys.stdout.write(\"\\033[F\\033[F\")\n",
    "    else:\n",
    "        # Print only the progress bar for burned samples\n",
    "        if thinning !=0:\n",
    "            print(progress_burned, end='\\r')\n",
    "\n",
    "    # If new_t is False, the cursor will remain on the same line ready for the next update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numerical_stability(tensor,tensor_name:str, dtype):\n",
    "        # Check for NaN values\n",
    "        has_nan = torch.isnan(tensor).any()\n",
    "        # Check for infinite values\n",
    "        has_inf = torch.isinf(tensor).any()\n",
    "        if (has_nan) | (has_inf):\n",
    "            print(f\"{tensor_name} has NaN values, consider increasing your current precision {dtype}\")\n",
    "            print(f\"{tensor_name} has Infinite value consider increasing your current precision {dtype}\")\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "\n",
    "def walk(\n",
    "        polytope:Union[Polytope, NFDPolytope], # Polytope Object\n",
    "        X0:torch.Tensor,    # Initial Interior point(s) of dim=(n,k).     \n",
    "                            #- If z > k (number of columns in X0), pad  by repeating vectors from X to fill z columns.\n",
    "                            #- If z < k, take the first z columns from X0.\n",
    "                            #- If z == k, equal to X0.\n",
    "        z:int=1, # The number of simultaneous to be executed (padding parameter).\n",
    "        T:int=1, # id-iterations, total_iid_points = T*z. Each iid iteration will burn the samples established by the thinning factor.\n",
    "        warm:int=None, # Number of iid-iterations needed to warm. The walk will execute warm steps before saving the points.\n",
    "        thinning:int=None, # Thinning Factor. Default O(n^3)\n",
    "        device:str = None, # Deveice to use, cpu or cuda\n",
    "        seed:int=None, # Seed for Pseudo-Random Number Generation\n",
    "        verbosity:int=1, # Verbosity of the execution\n",
    "        ) -> torch.Tensor:\n",
    "        \n",
    "        \n",
    "    ## Check validity \n",
    "    # Device\n",
    "    device = device if device is not None else polytope.device\n",
    "    assert(device in ['cpu', 'cuda']), print('The device is not correctly specified: ', device,\n",
    "                                        '\\n Please choose cpu or cuda')\n",
    "    # X0 dimension\n",
    "    assert(X0.shape[0] == polytope.n)\n",
    "    \n",
    "    ## Set min and max values\n",
    "    min_ = torch.finfo(polytope.dtype).min + 2\n",
    "    max_ = torch.finfo(polytope.dtype).max - 2\n",
    "    eps =  torch.finfo(polytope.dtype).eps\n",
    "    tolerance = eps*3\n",
    "    if verbosity > 1:\n",
    "        print(f'Minimum number allowed {min_}')\n",
    "        print(f'Maximum number allowed {max_}')\n",
    "        print(f'Eps: ', eps)\n",
    "        print(f'Values close to zero will be converted to 3eps or -3eps:', tolerance)\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## Set seed\n",
    "    random_gen = torch.Generator(device=device)\n",
    "    if seed:\n",
    "        random_gen.manual_seed(seed)\n",
    "    else:\n",
    "        random_gen.seed()\n",
    "    \n",
    "    \n",
    "    ## Check Dimensions\n",
    "    n = polytope.n\n",
    "    mI = polytope.mI\n",
    "    if isinstance(polytope, NFDPolytope) :\n",
    "        mE = polytope.mE\n",
    "    else:\n",
    "        mE=None            \n",
    "    if verbosity >=1:\n",
    "        print('n: ', n, '  mI:', mI, '  mE:', mE, '  z:', z)\n",
    "        \n",
    "    \n",
    "    ## Compute/set thinning factor\n",
    "    if thinning is not None:\n",
    "        pass\n",
    "    else:\n",
    "        thinning = int(n * n * n)\n",
    "        if verbosity >= 1:\n",
    "            print('Automatic Thinning factor: ', thinning)\n",
    "            \n",
    "    if warm is not None:\n",
    "        pass\n",
    "    else:\n",
    "        warm = thinning\n",
    "        if verbosity >= 1:\n",
    "            print('Warming iterations: ', thinning)\n",
    "     \n",
    "            \n",
    "    ## Prepare and send Matrices\n",
    "    init_x0 = sample_inner_points(X0,z).to(device)  \n",
    "    polytope.send_to_device(device)\n",
    "    \n",
    "    ## Iteration Loop\n",
    "    t = 1\n",
    "    burned = 0\n",
    "    dtype = polytope.dtype\n",
    "    x = init_x0.to(dtype)\n",
    "    X = torch.empty((T,n, z), dtype=dtype)\n",
    "    while t <= T:\n",
    "        h = create_h(n, z, generator=random_gen, dtype=dtype,device=device)\n",
    "        if isinstance(polytope, NFDPolytope) :\n",
    "            d = polytope.project_h(h)\n",
    "        else:\n",
    "            d = h\n",
    "            \n",
    "        ## Compute B - AX / AD\n",
    "        numerator = polytope.b_in - torch.matmul(polytope.A_in, x)\n",
    "        denominator = torch.matmul(polytope.A_in, d).to(dtype)\n",
    "        denominator = torch.where((denominator >= 0) & (denominator < tolerance), torch.full_like(denominator, tolerance), denominator)\n",
    "        denominator = torch.where((denominator < 0) & (denominator > -tolerance), torch.full_like(denominator, -tolerance), denominator)\n",
    "        check = check_numerical_stability(denominator,'Denominator (A_I)D ', dtype)\n",
    "        if check:\n",
    "            return denominator\n",
    "        \n",
    "        check = check_numerical_stability(numerator,'Numerator b_I - A_I(X) ', dtype)\n",
    "        if check:\n",
    "            return numerator\n",
    "        \n",
    "        # Overwrite numerator to keep memory free\n",
    "        frac = numerator / denominator\n",
    "        #frac = torch.clamp(frac, min=max_, max=max_)\n",
    "\n",
    "        check = check_numerical_stability(frac,'Fraction [b_I - A_I(X)]/[(A_I)D] ', dtype)\n",
    "        if check:\n",
    "            return frac, numerator, denominator\n",
    "        \n",
    "        # From the positive denominators you want the smallest one\n",
    "        lambda_pos = torch.min((denominator < 0.0).to(dtype) * max_ + # For negative denominator use max val\n",
    "                               (denominator > 0.0).to(dtype) * frac, 0).values.to(dtype)[None,:]\n",
    "        # From the negative denominators you want the biggest\n",
    "        lambda_neg = torch.max((denominator > 0.0).to(dtype) * min_ +\n",
    "                               (denominator < 0.0).to(dtype) * frac, 0).values.to(dtype)[None,:]\n",
    "        \n",
    "    \n",
    "        # Uniform draw\n",
    "        u = draw_uniform(z, generator=random_gen, dtype=dtype,device=polytope.device)\n",
    "        theta = (1.0 - u) * lambda_pos + u * lambda_neg\n",
    "\n",
    "        # New X\n",
    "        x = x.to(dtype) + d.to(dtype) * theta.to(dtype)\n",
    "        \n",
    "        if verbosity >=1:\n",
    "            update_progress_bars(burned, thinning, t, T)        \n",
    "        \n",
    "        if (warm > 0) & (burned >= thinning):\n",
    "            warm = warm - 1\n",
    "            burned = 0\n",
    "        \n",
    "        # Manage the burning rate and save points\n",
    "        if burned >= thinning:\n",
    "            X[t-1,:, :] = x.to('cpu')        \n",
    "            \n",
    "            \n",
    "            if verbosity >=1:\n",
    "                update_progress_bars(burned, thinning, t, T, new_t=True)\n",
    "            t = t + 1\n",
    "            burned = 0\n",
    "\n",
    "        burned = burned + 1\n",
    "\n",
    "    return X\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n",
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n",
      "Minimum number allowed -3.4028234663852886e+38\n",
      "Maximum number allowed 3.4028234663852886e+38\n",
      "Eps:  1.1920928955078125e-07\n",
      "Values close to zero will be converted to 3eps or -3eps: 3.5762786865234375e-07\n",
      "n:  10000   mI: 20000   mE: None   z: 100\n",
      "/tmp/ipykernel_41224/1586572147.py:29: UserWarning:\n",
      "  The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such\n",
      "  as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at\n",
      "  ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope_examples import Hypercube\n",
    "from mhar.inner_point import ChebyshevCenter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "hypercube = Hypercube(10000,\n",
    "                      dtype=torch.float32,\n",
    "                      device='cuda'\n",
    "                      )\n",
    "x0 = ChebyshevCenter(polytope=hypercube, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-4,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float32)\n",
    "\n",
    "X = walk(polytope=hypercube,\n",
    "        X0 = x0,  \n",
    "        z=100, \n",
    "        T=1, \n",
    "        warm=0,\n",
    "        thinning=10000, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite:  tensor(0)\n",
      "Nans:   tensor(0)\n",
      "Inequality violation:   tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print('Infinite: ', (~torch.isfinite(X)).sum())\n",
    "print('Nans:  ',(torch.isnan(X)).sum())\n",
    "print('Inequality violation:  ',(X.abs()>1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n",
      "Max non zero error for term (A A')^(-1)A at precision torch.float64:  tensor(0., device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n",
      "Minimum number allowed -1.7976931348623157e+308\n",
      "Maximum number allowed 1.7976931348623157e+308\n",
      "Eps:  2.220446049250313e-16\n",
      "Values close to zero will be converted to 3eps or -3eps: 6.661338147750939e-16\n",
      "n:  1000   mI: 1000   mE: 1   z: 100\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope_examples import Simplex\n",
    "from mhar.inner_point import ChebyshevCenter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "simplex = Simplex(\n",
    "    n=1000,\n",
    "    dtype=torch.float64,\n",
    "    copy=False,\n",
    "    device='cuda',\n",
    "    requires_grad=False\n",
    ")\n",
    "simplex.compute_projection_matrix(device='cuda')\n",
    "\n",
    "x0 = ChebyshevCenter(polytope=simplex, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-10,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float64)\n",
    "\n",
    "X = walk(polytope=simplex,\n",
    "        X0 = x0,  \n",
    "        z=100, \n",
    "        T=1, \n",
    "        warm=0,\n",
    "        thinning=10000000, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite:  tensor(0)\n",
      "Nans:   tensor(0)\n",
      "Inequality violation:   tensor(98)\n",
      "Inequality violation:   tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print('Infinite: ', (~torch.isfinite(X)).sum())\n",
    "print('Nans:  ',(torch.isnan(X)).sum())\n",
    "print('Inequality violation:  ',(X.sum(1)!=1.0).sum())\n",
    "print('Inequality violation:  ',((X.sum(1)- 1.0).abs() >.00000001).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inequality violation:   tensor(62)\n"
     ]
    }
   ],
   "source": [
    "print('Inequality violation:  ',((X.sum(1)- 1.0).abs() >0.0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inequality violation:   tensor(62)\n"
     ]
    }
   ],
   "source": [
    "print('Inequality violation:  ',(X.sum(1)!=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = x[0][0:,0].sum()\n",
    "(r.abs()-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4409e-16, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.matmul(A, torch.matmul(A_pinv,x[0][0:,0].view(1,-1))).sum() \n",
    "(r.abs()-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1000x1 and 1000x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Project x onto the column space of A\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Using the Moore-Penrose pseudoinverse for the general case\u001b[39;00m\n\u001b[1;32m     12\u001b[0m A_pinv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpinverse(A)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m x_col_proj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(A, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_pinv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Adjust the projection to satisfy Ax = b\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Note: x_proj already satisfies Ax = b\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# If you need x to be as close to x_proj as possible while satisfying Ax = b,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# you can use x_col_proj + (x_proj - A @ A_pinv @ x_proj)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# But here, we'll just project x onto the column space of A\u001b[39;00m\n\u001b[1;32m     20\u001b[0m x_final_proj \u001b[38;5;241m=\u001b[39m x_col_proj\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1000x1 and 1000x1)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your matrix A (size m x n), vector b (size m), and vector x (size n)\n",
    "A = simplex.A_eq\n",
    "b = simplex.b_eq\n",
    "x = X.to('cuda')\n",
    "\n",
    "# Solve the least squares problem to find x_proj\n",
    "x_proj = torch.linalg.lstsq(b, A)[0]\n",
    "x_proj = x_proj[:A.size(1)]\n",
    "\n",
    "# Project x onto the column space of A\n",
    "# Using the Moore-Penrose pseudoinverse for the general case\n",
    "A_pinv = torch.pinverse(A).to('cuda')\n",
    "x_col_proj = torch.matmul(A, torch.matmul(A_pinv,x[0][0:,0].view(-1,1)))\n",
    "\n",
    "# Adjust the projection to satisfy Ax = b\n",
    "# Note: x_proj already satisfies Ax = b\n",
    "# If you need x to be as close to x_proj as possible while satisfying Ax = b,\n",
    "# you can use x_col_proj + (x_proj - A @ A_pinv @ x_proj)\n",
    "# But here, we'll just project x onto the column space of A\n",
    "x_final_proj = x_col_proj\n",
    "\n",
    "# x_final_proj is now the projection of x onto the space defined by Ax = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "        ...,\n",
       "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         ...,\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_matrix = torch.eye(X.shape[1]).to('cuda') - simplex.projection_matrix\n",
    "torch.matmul(correction_matrix.to('cpu').double(),X.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         ...,\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
       "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_matrix = torch.eye(X.shape[1]).to('cuda') - simplex.projection_matrix\n",
    "torch.matmul(correction_matrix.to('cpu').double(),X.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inequality violation:   tensor(3.9635e-14, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "correction_matrix = torch.eye(X.shape[1]).to('cuda') - simplex.projection_matrix\n",
    "Y = torch.matmul(correction_matrix.to('cpu'),X)\n",
    "\n",
    "print('Inequality violation:  ',(Y.sum(1)-1).sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
