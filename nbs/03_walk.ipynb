{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHAR walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mhar.polytope import Polytope, NFDPolytope\n",
    "from mhar.utils import print_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sample_inner_points(X0, z):\n",
    "    n, k = X0.shape\n",
    "    if z > k:\n",
    "        # Pad by repeating vectors from X to fill z columns\n",
    "        num_repeats = z // k\n",
    "        remainder = z % k\n",
    "        I = torch.cat([X0] * num_repeats + [X0[:, :remainder]], dim=1)\n",
    "    elif z < k:\n",
    "        # Take the first z columns from X\n",
    "        I = X0[:, :z]\n",
    "    else:\n",
    "        # z == k, I equals X\n",
    "        I = X0.clone()\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_h(n,z,generator,dtype,device):\n",
    "    \"\"\"\n",
    "    Creates a Tensor (z x n x 1) where each entry ~ N(0,1). Automatically detects the\n",
    "    precision 64 bits or 32 bits.\n",
    "    -------------\n",
    "    :param n:   int\n",
    "                Dimension of the Space where the Polytope Lives\n",
    "    :param z:   int\n",
    "                Padding Parameter\n",
    "    :param generator:\n",
    "    :param device:  String, default = cpu\n",
    "                    Hardware used to make the computations and allocate the result.\n",
    "                    If equal to cpu then the CPUs are used for computing the inverse.\n",
    "                    If equal to cuda then the a GPU is used for computing the inverse.\n",
    "    -------------\n",
    "    :return:    Torch Tensor\n",
    "                Tensor (z x n x 1) where each entry ~ N(0,1)Contains a tensor\n",
    "\n",
    "    \"\"\"\n",
    "    if '64' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            h = torch.cuda.DoubleTensor(n, z).normal_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            h = torch.DoubleTensor(n, z).normal_(generator=generator)\n",
    "    elif '32' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            h = torch.cuda.FloatTensor(n, z).normal_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            h = torch.FloatTensor(n, z).normal_(generator=generator)\n",
    "    elif '16' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            h = torch.cuda.HalfTensor(n, z).normal_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            h = torch.HalfTensor(n, z).normal_(generator=generator)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def draw_uniform(z, generator,dtype,device='cpu'):\n",
    "    \"\"\"\n",
    "    Creates a tensor (z x 1) where each entry ~ U(0,1). Automatically detects the\n",
    "    precision 64 bits or 32 bits.\n",
    "    -------------\n",
    "    :param n:   int\n",
    "                Dimension of the Space where the Polytope Lives\n",
    "    :param z:   int\n",
    "                Padding Parameter\n",
    "    :param generator:\n",
    "    :param device:  String, default = cpu\n",
    "                    Hardware used to make the computations and allocate the result.\n",
    "                    If equal to cpu then the CPUs are used for computing the inverse.\n",
    "                    If equal to cuda then the a GPU is used for computing the inverse.\n",
    "    -------------\n",
    "    :return:    Torch Tensor\n",
    "                Tensor (z x 1) where each entry ~ U(0,1)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if '64' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            u = torch.cuda.DoubleTensor(1, z).uniform_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            u = torch.DoubleTensor(1,z).uniform_(generator=generator)\n",
    "    elif '32' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            u = torch.cuda.FloatTensor(1, z).uniform_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            u = torch.FloatTensor(1, z).uniform_(generator=generator)\n",
    "    elif '16' in str(dtype):\n",
    "        if 'cuda' in device:\n",
    "            u = torch.cuda.HalfTensor(1, z).uniform_(generator=generator)\n",
    "        elif 'cpu' == device:\n",
    "            u = torch.HalfTensor(1, z).uniform_(generator=generator)\n",
    "\n",
    "    return u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "\n",
    "def update_progress_bars(burned, thinning, t, T, new_t: bool = False):\n",
    "    # Generate progress bar string for burned samples\n",
    "    progress_burned = print_progress(iteration=burned, prefix='% of burned samples', total=thinning, length=30)\n",
    "\n",
    "    # Clear the current line\n",
    "    sys.stdout.write('\\033[K')\n",
    "\n",
    "    if new_t:\n",
    "        # Generate progress bar string for iid samples\n",
    "        progress_iid = print_progress(iteration=t, prefix='% of iid samples', total=T, length=30)\n",
    "        # Print both progress bars\n",
    "        print(progress_burned, end='\\n')\n",
    "        print(progress_iid, end='\\n')\n",
    "        # Move cursor up two lines for the next update\n",
    "        #sys.stdout.write(\"\\033[F\\033[F\")\n",
    "    else:\n",
    "        # Print only the progress bar for burned samples\n",
    "        print(progress_burned, end='\\r')\n",
    "\n",
    "    # If new_t is False, the cursor will remain on the same line ready for the next update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "\n",
    "def walk(\n",
    "        polytope:Union[Polytope, NFDPolytope], # Polytope Object\n",
    "        X0:torch.Tensor,    # Initial Interior point(s) of dim=(n,k).     \n",
    "                            #- If z > k (number of columns in X0), pad  by repeating vectors from X to fill z columns.\n",
    "                            #- If z < k, take the first z columns from X0.\n",
    "                            #- If z == k, equal to X0.\n",
    "        z:int=1, # The number of simultaneous to be executed (padding parameter).\n",
    "        T:int=1, # id-iterations, total_iid_points = T*z. Each iid iteration will burn the samples established by the thinning factor.\n",
    "        warm:int=None, # Number of iid-iterations needed to warm. The walk will execute warm steps before saving the points.\n",
    "        thinning:int=None, # Thinning Factor. Default O(n^3)\n",
    "        device:str = None, # Deveice to use, cpu or cuda\n",
    "        seed:int=None, # Seed for Pseudo-Random Number Generation\n",
    "        verbosity:int=1, # Verbosity of the execution\n",
    "        ) -> torch.Tensor:\n",
    "        \n",
    "        \n",
    "    ## Check validity \n",
    "    # Device\n",
    "    device = device if device is not None else polytope.device\n",
    "    assert(device in ['cpu', 'cuda']), print('The device is not correctly specified: ', device,\n",
    "                                        '\\n Please choose cpu or cuda')\n",
    "    # X0 dimension\n",
    "    assert(X0.shape[0] == polytope.n)\n",
    "    \n",
    "    ## Set min and max values\n",
    "    min_ = torch.finfo(polytope.dtype).min + 2.0\n",
    "    max_ = torch.finfo(polytope.dtype).max - 2.0\n",
    "    if verbosity > 1:\n",
    "        print(f'Minimum number allowed {min_}')\n",
    "        print(f'Maximum number allowed {max_}')\n",
    "    \n",
    "    \n",
    "    ## Set seed\n",
    "    random_gen = torch.Generator(device=device)\n",
    "    if seed:\n",
    "        random_gen.manual_seed(seed)\n",
    "    else:\n",
    "        random_gen.seed()\n",
    "    \n",
    "    \n",
    "    ## Check Dimensions\n",
    "    n = polytope.n\n",
    "    mI = polytope.mI\n",
    "    if isinstance(polytope, NFDPolytope) :\n",
    "        mE = polytope.mE\n",
    "    else:\n",
    "        mE=None            \n",
    "    if verbosity >=1:\n",
    "        print('n: ', n, '  mI:', mI, '  mE:', mE, '  z:', z)\n",
    "        \n",
    "    \n",
    "    ## Compute/set thinning factor\n",
    "    if thinning is not None:\n",
    "        pass\n",
    "    else:\n",
    "        thinning = int(n * n * n)\n",
    "        if verbosity >= 1:\n",
    "            print('Automatic Thinning factor: ', thinning)\n",
    "            \n",
    "    if warm is not None:\n",
    "        pass\n",
    "    else:\n",
    "        warm = thinning\n",
    "        if verbosity >= 1:\n",
    "            print('Warming iterations: ', thinning)\n",
    "     \n",
    "            \n",
    "    ## Prepare and send Matrices\n",
    "    init_x0 = sample_inner_points(X0,z).to(device)  \n",
    "    polytope.send_to_device(device)\n",
    "    \n",
    "    ## Iteration Loop\n",
    "    t = 1\n",
    "    burned = 0\n",
    "    dtype = polytope.dtype\n",
    "    x = init_x0\n",
    "    X = torch.empty((T,n, z))\n",
    "    while t <= T:\n",
    "        h = create_h(n, z, generator=random_gen, dtype=dtype,device=device)\n",
    "        if isinstance(polytope, NFDPolytope) :\n",
    "            d = polytope.project_h(h)\n",
    "        else:\n",
    "            d = h\n",
    "        #print('H',h.shape)\n",
    "        # Compute B - AX / AD\n",
    "\n",
    "        numerator = polytope.b_in - torch.matmul(polytope.A_in, x)\n",
    "        #print('numerator',numerator.shape)\n",
    "        denominator = torch.matmul(polytope.A_in, d)\n",
    "        #print('denomitanr',denominator.shape)\n",
    "        \n",
    "        # Overwrite numerator to keep memory free\n",
    "        numerator = numerator / denominator\n",
    "        # From the positive denominators you want the smallest one\n",
    "        lambda_pos = torch.min(~(denominator > 0.0) * max_ +\n",
    "                               (denominator > 0.0) * numerator, 0).values.to(dtype)[None,:]\n",
    "        # From the negative denominators you want the biggest\n",
    "        lambda_neg = torch.max(~(denominator < 0.0) * min_ +\n",
    "                               (denominator < 0.0) * numerator, 0).values.to(dtype)[None,:]\n",
    "        \n",
    "    \n",
    "        # Uniform draw\n",
    "        u = draw_uniform(z, generator=random_gen, dtype=dtype,device=polytope.device)\n",
    "        # print('u',u.shape)\n",
    "        # print('lambda_pos',lambda_pos.shape)\n",
    "        # print('lambda_neg',lambda_neg.shape)\n",
    "        \n",
    "        theta = (1.0 - u) * lambda_pos + u * lambda_neg\n",
    "        # print('theta', theta.shape)\n",
    "        # print('d', d.shape)\n",
    "        # print('theta_2', (d * theta).shape)\n",
    "        \n",
    "        # New X\n",
    "        x = x + d * theta\n",
    "        #print('x: ',x.shape) \n",
    "        if verbosity >=1:\n",
    "            update_progress_bars(burned, thinning, t, T)        \n",
    "        \n",
    "        if (warm > 0) & (burned >= thinning):\n",
    "            warm = warm - 1\n",
    "            burned = 0\n",
    "        \n",
    "        # Manage the burning rate and save points\n",
    "        if burned >= thinning:\n",
    "            X[t-1,:, :] = x.to('cpu')        \n",
    "            \n",
    "            \n",
    "            if verbosity >=1:\n",
    "                update_progress_bars(burned, thinning, t, T, new_t=True)\n",
    "            t = t + 1\n",
    "            burned = 0\n",
    "\n",
    "        burned = burned + 1\n",
    "\n",
    "    return X\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:97: UserWarning:\n",
      "  The dtype torch.float16 is typically used with GPU architectures. If you are using CPU, consider\n",
      "  using 32 or 64-bit dtypes. Certain operations may be casted to 32 or 64 bits to enhance numerical\n",
      "  stability.\n",
      "\n",
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n",
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n",
      "n:  50   mI: 100   mE: None   z: 2\n",
      "/tmp/ipykernel_22778/1586572147.py:34: UserWarning:\n",
      "  The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such\n",
      "  as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at\n",
      "  ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███---------------------------| 10.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████------------------------| 20.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |█████████---------------------| 30.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |████████████------------------| 40.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███████████████---------------| 50.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████------------| 60.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |█████████████████████---------| 70.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |████████████████████████------| 80.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███████████████████████████---| 90.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope_examples import Hypercube\n",
    "from mhar.inner_point import ChebyshevCenter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "hypercube = Hypercube(50,\n",
    "                      dtype=torch.float16,\n",
    "                      device='cuda'\n",
    "                      )\n",
    "x0 = ChebyshevCenter(polytope=hypercube, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-4,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float64)\n",
    "\n",
    "X = walk(polytope=hypercube,\n",
    "        X0 = x0,  \n",
    "        z=2, \n",
    "        T=10, \n",
    "        warm=0,\n",
    "        thinning=3, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n",
      "Max non zero error for term (A A')^(-1)A at precision torch.float32:  tensor(0., device='cuda:0')\n",
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n",
      "n:  1000   mI: 1000   mE: 1   z: 2\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███---------------------------| 10.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████------------------------| 20.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |█████████---------------------| 30.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |████████████------------------| 40.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███████████████---------------| 50.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████------------| 60.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |█████████████████████---------| 70.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |████████████████████████------| 80.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███████████████████████████---| 90.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope_examples import Simplex\n",
    "from mhar.inner_point import ChebyshevCenter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "simplex = Simplex(\n",
    "    n=1000,\n",
    "    dtype=torch.float32,\n",
    "    copy=False,\n",
    "    device='cuda',\n",
    "    requires_grad=False\n",
    ")\n",
    "simplex.compute_projection_matrix(device='cuda')\n",
    "\n",
    "x0 = ChebyshevCenter(polytope=simplex, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-10,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float64)\n",
    "\n",
    "X = walk(polytope=simplex,\n",
    "        X0 = x0,  \n",
    "        z=2, \n",
    "        T=10, \n",
    "        warm=0,\n",
    "        thinning=10000, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
