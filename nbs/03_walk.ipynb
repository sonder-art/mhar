{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHAR walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mhar.polytope import Polytope, NFDPolytope\n",
    "from mhar.utils import print_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def sample_inner_points(X0, z):\n",
    "    n, k = X0.shape\n",
    "    if z > k:\n",
    "        # Pad by repeating vectors from X to fill z columns\n",
    "        num_repeats = z // k\n",
    "        remainder = z % k\n",
    "        I = torch.cat([X0] * num_repeats + [X0[:, :remainder]], dim=1)\n",
    "    elif z < k:\n",
    "        # Take the first z columns from X\n",
    "        I = X0[:, :z]\n",
    "    else:\n",
    "        # z == k, I equals X\n",
    "        I = X0.clone()\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_h(n,z,generator,dtype,device):\n",
    "    \"\"\"\n",
    "    Creates a Tensor (z x n x 1) where each entry ~ N(0,1). Automatically detects the\n",
    "    precision 64 bits or 32 bits.\n",
    "    -------------\n",
    "    :param n:   int\n",
    "                Dimension of the Space where the Polytope Lives\n",
    "    :param z:   int\n",
    "                Padding Parameter\n",
    "    :param generator:\n",
    "    :param device:  String, default = cpu\n",
    "                    Hardware used to make the computations and allocate the result.\n",
    "                    If equal to cpu then the CPUs are used for computing the inverse.\n",
    "                    If equal to cuda then the a GPU is used for computing the inverse.\n",
    "    -------------\n",
    "    :return:    Torch Tensor\n",
    "                Tensor (z x n x 1) where each entry ~ N(0,1)Contains a tensor\n",
    "\n",
    "    \"\"\"\n",
    "    h = torch.randn((n, z), dtype=dtype, device=device, generator=generator)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def draw_uniform(z, generator,dtype,device='cpu'):\n",
    "    \"\"\"\n",
    "    Creates a tensor (z x 1) where each entry ~ U(0,1). Automatically detects the\n",
    "    precision 64 bits or 32 bits.\n",
    "    -------------\n",
    "    :param n:   int\n",
    "                Dimension of the Space where the Polytope Lives\n",
    "    :param z:   int\n",
    "                Padding Parameter\n",
    "    :param generator:\n",
    "    :param device:  String, default = cpu\n",
    "                    Hardware used to make the computations and allocate the result.\n",
    "                    If equal to cpu then the CPUs are used for computing the inverse.\n",
    "                    If equal to cuda then the a GPU is used for computing the inverse.\n",
    "    -------------\n",
    "    :return:    Torch Tensor\n",
    "                Tensor (z x 1) where each entry ~ U(0,1)\n",
    "\n",
    "    \"\"\"\n",
    "    u = torch.rand((1, z), dtype=dtype, device=device, generator=generator)\n",
    "\n",
    "    return u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "\n",
    "def update_progress_bars(burned, thinning, t, T, new_t: bool = False):\n",
    "    # Generate progress bar string for burned samples\n",
    "    if thinning !=0:\n",
    "        progress_burned = print_progress(iteration=burned, prefix='% of burned samples', total=thinning, length=30)\n",
    "\n",
    "    # Clear the current line\n",
    "    sys.stdout.write('\\033[K')\n",
    "\n",
    "    if new_t:\n",
    "        # Generate progress bar string for iid samples\n",
    "        progress_iid = print_progress(iteration=t, prefix='% of iid samples', total=T, length=30)\n",
    "        # Print both progress bars\n",
    "        if thinning !=0:\n",
    "            print(progress_burned, end='\\n')\n",
    "        print(progress_iid, end='\\n')\n",
    "        # Move cursor up two lines for the next update\n",
    "        #sys.stdout.write(\"\\033[F\\033[F\")\n",
    "    else:\n",
    "        # Print only the progress bar for burned samples\n",
    "        if thinning !=0:\n",
    "            print(progress_burned, end='\\r')\n",
    "\n",
    "    # If new_t is False, the cursor will remain on the same line ready for the next update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_numerical_stability(tensor,tensor_name:str, dtype):\n",
    "        # Check for NaN values\n",
    "        has_nan = torch.isnan(tensor).any()\n",
    "        # Check for infinite values\n",
    "        has_inf = torch.isinf(tensor).any()\n",
    "        if (has_nan) | (has_inf):\n",
    "            print(f\"{tensor_name} has NaN values, consider increasing your current precision {dtype}\")\n",
    "            print(f\"{tensor_name} has Infinite value consider increasing your current precision {dtype}\")\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def walk(\n",
    "        polytope:Union[Polytope, NFDPolytope], # Polytope Object\n",
    "        X0:torch.Tensor,    # Initial Interior point(s) of dim=(n,k).     \n",
    "                            #- If z > k (number of columns in X0), pad  by repeating vectors from X to fill z columns.\n",
    "                            #- If z < k, take the first z columns from X0.\n",
    "                            #- If z == k, equal to X0.\n",
    "        z:int=1, # The number of simultaneous to be executed (padding parameter).\n",
    "        T:int=1, # id-iterations, total_iid_points = T*z. Each iid iteration will burn the samples established by the thinning factor.\n",
    "        warm:int=None, # Number of iid-iterations needed to warm. The walk will execute warm steps before saving the points.\n",
    "        thinning:int=None, # Thinning Factor. Default O(n^3)\n",
    "        device:str = None, # Deveice to use, cpu or cuda\n",
    "        seed:int=None, # Seed for Pseudo-Random Number Generation\n",
    "        verbosity:int=1, # Verbosity of the execution\n",
    "        ) -> torch.Tensor:\n",
    "        \n",
    "        \n",
    "    ## Check validity \n",
    "    # Device\n",
    "    device = device if device is not None else polytope.device\n",
    "    assert(device in ['cpu', 'cuda']), print('The device is not correctly specified: ', device,\n",
    "                                        '\\n Please choose cpu or cuda')\n",
    "    # X0 dimension\n",
    "    assert(X0.shape[0] == polytope.n)\n",
    "    \n",
    "    ## Set min and max values\n",
    "    min_ = torch.finfo(polytope.dtype).min + 2\n",
    "    max_ = torch.finfo(polytope.dtype).max - 2\n",
    "    eps =  torch.finfo(polytope.dtype).eps\n",
    "    tolerance = eps*3\n",
    "    if verbosity > 1:\n",
    "        print(f'Minimum number allowed {min_}')\n",
    "        print(f'Maximum number allowed {max_}')\n",
    "        print(f'Eps: ', eps)\n",
    "        print(f'Values close to zero will be converted to 3eps or -3eps:', tolerance)\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## Set seed\n",
    "    random_gen = torch.Generator(device=device)\n",
    "    if seed:\n",
    "        random_gen.manual_seed(seed)\n",
    "    else:\n",
    "        random_gen.seed()\n",
    "    \n",
    "    \n",
    "    ## Check Dimensions\n",
    "    n = polytope.n\n",
    "    mI = polytope.mI\n",
    "    if isinstance(polytope, NFDPolytope) :\n",
    "        mE = polytope.mE\n",
    "    else:\n",
    "        mE=None            \n",
    "    if verbosity >=1:\n",
    "        print('n: ', n, '  mI:', mI, '  mE:', mE, '  z:', z)\n",
    "        \n",
    "    \n",
    "    ## Compute/set thinning factor\n",
    "    if thinning is not None:\n",
    "        pass\n",
    "    else:\n",
    "        thinning = int(n * n * n)\n",
    "        if verbosity >= 1:\n",
    "            print('Automatic Thinning factor: ', thinning)\n",
    "            \n",
    "    if warm is not None:\n",
    "        pass\n",
    "    else:\n",
    "        warm = thinning\n",
    "        if verbosity >= 1:\n",
    "            print('Warming iterations: ', thinning)\n",
    "     \n",
    "            \n",
    "    ## Prepare and send Matrices\n",
    "    init_x0 = sample_inner_points(X0,z).to(device)  \n",
    "    polytope.send_to_device(device)\n",
    "    \n",
    "    ## Iteration Loop\n",
    "    t = 1\n",
    "    burned = 0\n",
    "    dtype = polytope.dtype\n",
    "    x = init_x0.to(dtype)\n",
    "    X = torch.empty((T,n, z), dtype=dtype)\n",
    "    \n",
    "    while t <= T:\n",
    "        \n",
    "        h = create_h(n, z, generator=random_gen, dtype=dtype,device=device)        \n",
    "        if isinstance(polytope, NFDPolytope) :\n",
    "            d = polytope.project_h(h)\n",
    "        else:\n",
    "            d = h\n",
    "        ## Compute B - AX / AD\n",
    "        numerator = polytope.b_in - torch.matmul(polytope.A_in, x)\n",
    "        denominator = torch.matmul(polytope.A_in, d).to(dtype)\n",
    "        denominator = torch.where((denominator >= 0) & (denominator < tolerance), torch.full_like(denominator, tolerance), denominator)\n",
    "        denominator = torch.where((denominator < 0) & (denominator > -tolerance), torch.full_like(denominator, -tolerance), denominator)\n",
    "        check = check_numerical_stability(denominator,'Denominator (A_I)D ', dtype)\n",
    "        if check:\n",
    "            return denominator\n",
    "        \n",
    "        check = check_numerical_stability(numerator,'Numerator b_I - A_I(X) ', dtype)\n",
    "        if check:\n",
    "            return numerator\n",
    "        \n",
    "        # Overwrite numerator to keep memory free\n",
    "        frac = numerator / denominator\n",
    "        #frac = torch.clamp(frac, min=max_, max=max_)\n",
    "\n",
    "        check = check_numerical_stability(frac,'Fraction [b_I - A_I(X)]/[(A_I)D] ', dtype)\n",
    "        if check:\n",
    "            return frac, numerator, denominator\n",
    "        \n",
    "        # From the positive denominators you want the smallest one\n",
    "        lambda_pos = torch.min((denominator < 0.0).to(dtype) * max_ + # For negative denominator use max val\n",
    "                               (denominator > 0.0).to(dtype) * frac, 0).values.to(dtype)[None,:]\n",
    "        # From the negative denominators you want the biggest\n",
    "        lambda_neg = torch.max((denominator > 0.0).to(dtype) * min_ +\n",
    "                               (denominator < 0.0).to(dtype) * frac, 0).values.to(dtype)[None,:]\n",
    "        \n",
    "    \n",
    "        # Uniform draw\n",
    "        u = draw_uniform(z, generator=random_gen, dtype=dtype,device=polytope.device)\n",
    "        theta = (1.0 - u) * lambda_pos + u * lambda_neg\n",
    "\n",
    "        # New X\n",
    "        x = x.to(dtype) + d.to(dtype) * theta.to(dtype)\n",
    "        \n",
    "        if verbosity >=1:\n",
    "            update_progress_bars(burned, thinning, t, T)        \n",
    "        \n",
    "        if (warm > 0) & (burned >= thinning):\n",
    "            warm = warm - 1\n",
    "            burned = 0\n",
    "        \n",
    "        # Manage the burning rate and save points\n",
    "        if burned >= thinning:\n",
    "            X[t-1,:, :] = x.to('cpu')        \n",
    "            \n",
    "            \n",
    "            if verbosity >=1:\n",
    "                update_progress_bars(burned, thinning, t, T, new_t=True)\n",
    "            t = t + 1\n",
    "            burned = 0\n",
    "\n",
    "        burned = burned + 1\n",
    "\n",
    "    return X\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhar.polytope_examples import Hypercube\n",
    "from mhar.inner_point import ChebyshevCenter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "hypercube = Hypercube(10000,\n",
    "                      dtype=torch.float32,\n",
    "                      device='cuda'\n",
    "                      )\n",
    "x0 = ChebyshevCenter(polytope=hypercube, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-4,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float32)\n",
    "\n",
    "X = walk(polytope=hypercube,\n",
    "        X0 = x0,  \n",
    "        z=100, \n",
    "        T=1, \n",
    "        warm=0,\n",
    "        thinning=10000, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Infinite: ', (~torch.isfinite(X)).sum())\n",
    "print('Nans:  ',(torch.isnan(X)).sum())\n",
    "print('Inequality violation:  ',(X.abs()>1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder.art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n",
      "Max non zero error for term (A A')^(-1)A at precision torch.float64:  tensor(0., device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n",
      "Minimum number allowed -1.7976931348623157e+308\n",
      "Maximum number allowed 1.7976931348623157e+308\n",
      "Eps:  2.220446049250313e-16\n",
      "Values close to zero will be converted to 3eps or -3eps: 6.661338147750939e-16\n",
      "n:  1000   mI: 1000   mE: 1   z: 1000\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope_examples import Simplex\n",
    "from mhar.inner_point import ChebyshevCenter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "simplex = Simplex(\n",
    "    n=1000,\n",
    "    dtype=torch.float64,\n",
    "    copy=False,\n",
    "    device='cuda',\n",
    "    requires_grad=False\n",
    ")\n",
    "simplex.compute_projection_matrix(device='cuda')\n",
    "\n",
    "x0 = ChebyshevCenter(polytope=simplex, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-10,\n",
    "                    device='cuda',\n",
    "                    solver_precision=np.float64)\n",
    "\n",
    "X = walk(polytope=simplex,\n",
    "        X0 = x0,  \n",
    "        z=1000, \n",
    "        T=1, \n",
    "        warm=0,\n",
    "        thinning=100000, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite:  tensor(0)\n",
      "Nans:   tensor(0)\n",
      "Inequality violation:   tensor(434)\n",
      "Inequality violation:   tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print('Infinite: ', (~torch.isfinite(X)).sum())\n",
    "print('Nans:  ',(torch.isnan(X)).sum())\n",
    "print('Inequality violation:  ',(X.sum(1)!=1.0).sum())\n",
    "print('Inequality violation:  ',((X.sum(1)- 1.0).abs() >1e-10).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
