{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sonder-art/mhar/blob/released/nbs/tutorial_nonfull_dimensional.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the next snippet if you are in colab or need to install mhar library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#!pip install mhar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will show how to use `mhar` for sampling non-full dimensional polytopes. It is focused on executing parallel MCMC walks over a polytope in GPUs. If you want to se a tutorial on how to sample full dimensional polytopes see [tutorial](https://github.com/sonder-art/mhar/blob/released/nbs/tutorial_full_dimensional.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting let's check if you have an avaialble gpu device or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to decide the data-type `dtype` we are going to use. Depending on your necessities you can choose it, we recomend to use `64` bits for non-fully dimentional polytopes in order to maintain numerical inestability of the projections. Otherwiise the precision depends on the dimension of your polytope and speed you want.  \n",
    "  \n",
    "As of now `16` bit precision is only available for `gpu` and not `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# We will choose 64-bits\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polytope in question must be presented in matrix canonical representation (as opposed to vertex). `mhar` assumes that the matrix has no repeated or redundant restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Fully dimensional Polytopes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $A^IX \\leq b^I$  \n",
    "> \n",
    "> $A^EX = b^E$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-fully dimensional polytopes we need to use the class [`NFDPolytope`](https://sonder-art.github.io/mhar/polytope.html#nfdpolytope) in the `mhar.polytope` module. The restrictions must be passed as pytorch tensors.  \n",
    "  \n",
    "We will sample the unit hypercube that is defined as:  \n",
    "> $n-simplex = \\{x \\in R^n || \\sum_{i=1}^{n} x_i = 1, 0 \\leq x_i \\} $  \n",
    "\n",
    "Which we can represent in matrix restrictions:  \n",
    "$ -Ix \\leq 0$  \n",
    "$ [1]^n = 1 $  \n",
    "Where $I$ is the identity matrix of dimension $n \\times n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the tensors to represent the restrictions that define the polytope. Since we need to create a projection matrix for the non-fully dimensional object we need to preserve the numerical stability of the algorithm, we suggest using 64 bits precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inequality Matrix A^I \n",
      " tensor([[-1., -0., -0.],\n",
      "        [-0., -1., -0.],\n",
      "        [-0., -0., -1.]], dtype=torch.float64) \n",
      "\n",
      "Inequality Vector b^I \n",
      " tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n",
      "\n",
      "Equality Matrix A^E \n",
      " tensor([[1., 1., 1.]], dtype=torch.float64) \n",
      "\n",
      "Equality Vector b^E \n",
      " tensor([[1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = 3 # Dimension\n",
    "dtype = torch.float64 # Precision \n",
    "A_I = torch.eye(n).to(dtype) * -1.0\n",
    "b_I = torch.empty(n, 1, dtype=dtype)\n",
    "b_I.fill_(0.0)\n",
    "\n",
    "# Create Equalities\n",
    "A_E = torch.empty(1, n, dtype=dtype)\n",
    "A_E.fill_(1.0)\n",
    "b_E = torch.empty(1, 1, dtype=dtype)\n",
    "b_E.fill_(1.0)      \n",
    "print(f'Inequality Matrix A^I \\n {A_I} \\n')\n",
    "print(f'Inequality Vector b^I \\n {b_I}\\n')\n",
    "print(f'Equality Matrix A^E \\n {A_E} \\n')\n",
    "print(f'Equality Vector b^E \\n {b_E}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a [`NFDPolytope`](https://sonder-art.github.io/mhar/polytope.html#nfdpolytope) object to represent the polytope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder-art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope import NFDPolytope\n",
    "simplex = NFDPolytope(A_I, # Inequality Restriction Matrix \n",
    "                     b_I,  # Inequality Vector\n",
    "                     A_E, # Equality Restriction Matrix \n",
    "                     b_E,  # Equality Vector\n",
    "                     dtype, # torch dtype\n",
    "                     device, # device used cpu or cuda\n",
    "                     copy=False # bool for creating a copy of the restrictions\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Numeric Precision (dtype) torch.float64\n",
       "Device: cuda\n",
       "A_in: torch.Size([3, 3]) \n",
       "b_in: torch.Size([3, 1])\n",
       "A_eq: torch.Size([1, 3]) \n",
       "b_eq: torch.Size([1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compute que projection matrix that we will use for projecting the random directions vectors to the equality space. For that we can use the method `NFDPolytope.compute_projection_matrix()`. We recommend using the highest precision possible to compute this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max non zero error for term (A A')^(-1)A at precision torch.float64:  tensor(0., device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "simplex.compute_projection_matrix(device=device, solver_precision=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Numeric Precision (dtype) torch.float64\n",
       "Device: cuda\n",
       "A_in: torch.Size([3, 3]) \n",
       "b_in: torch.Size([3, 1])\n",
       "A_eq: torch.Size([1, 3]) \n",
       "b_eq: torch.Size([1, 1])\n",
       "Projection Matrix: torch.Size([3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Inner Point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start the algorithm we need at least one inner point $x_0$. If you know your inner point you can supply it to the algorithm, `mhar` also contains functions to compute one inner point using the [chebyshev center](https://en.wikipedia.org/wiki/Chebyshev_center) which finds the center of the smallest ball inside the polytope.\n",
    "\n",
    " `from mhar.inner_point import ChebyshevCenter`. The solver is in numpy so precision must be specified as `numpy.dtype`. It uses `linprog` from `scipy.optimize`. You can see the documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html). \n",
    "  \n",
    "It could also be the last points produced by a previous walk/run of the `mhar` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from mhar.inner_point import ChebyshevCenter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n"
     ]
    }
   ],
   "source": [
    "x0 = ChebyshevCenter(polytope=simplex, # Polytope Object\n",
    "                    lb=None,  # Lowerbound (lb <= x ), if unknown leave it as None \n",
    "                    ub=None,  # Upperbound ( x <= up), if unknown leave it as None \n",
    "                    tolerance=1e-4, # Tolerance for equality restrictions (A_eqx = b_eq)\n",
    "                    device=device, # device used cpu or cuda\n",
    "                    solver_precision=np.float64 # numpy dtype\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333],\n",
       "        [0.3333],\n",
       "        [0.3333]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to manually input the inner points then it is enough to use a torch tensor of size $n \\times l$. Where $l$ is ne number of inner points you want to supply. Just write them in column notation.  \n",
    "  \n",
    "We are going to manually add an other starting point to the one calcualted by the `chebyshev center` to show its functionality later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.2500],\n",
       "        [0.3333, 0.2500],\n",
       "        [0.3333, 0.5000]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.cat([x0, \n",
    "            torch.tensor([[.25], [.25], [.5]]).to(device).to(dtype)\n",
    "             ], dim=1)\n",
    "x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to sample the `polytope`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to sample the polytope starting from the inner points we supply using the method `walk.walk`. It has the next arguments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `polytope` is an object of the type [`Polytope`](https://sonder-art.github.io/mhar/polytope.html#polytope) or [`NFDPolytope`](https://sonder-art.github.io/mhar/polytope.html#nfdpolytope) that defines it.\n",
    "+ `X0` a tensor containing the inner points to start the walks from.\n",
    "+ `z` determines the number of simoultaneous `walks`. If the number of initial points supplied are less than `z`  ($ncols($ `x0` $) < $ `z`) then some points will be reused as starting points.  \n",
    "+ `T` is the number of uncorrelated iterations you want. The number of total uncorrelated points produced by the algorithm is `z` $\\times$ `T`, since `z` points are sampled at each iteration.  \n",
    "+ `thinning` determines the number of points that we need to burn between iterations in order to get uncorrelated points. The suggested factor should be in the order of $O(n^3)$.\n",
    "+ `warm` determines a thinning for warming the walks only at the beggining, after the this war the walks resumes as normal. It is used if you want to lose the dependency from the starting points.\n",
    "+ `device` device where the tenros live `cpu` or `cuda`\n",
    "+ `seed` for reproducibility\n",
    "+ `verbosity` for printing what is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.2500],\n",
       "        [0.3333, 0.2500],\n",
       "        [0.3333, 0.5000]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number allowed -1.7976931348623157e+308\n",
      "Maximum number allowed 1.7976931348623157e+308\n",
      "Eps:  2.220446049250313e-16\n",
      "Values close to zero will be converted to 3eps or -3eps: 6.661338147750939e-16\n",
      "n:  3   mI: 3   mE: 1   z: 100\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███---------------------------| 10.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████------------------------| 20.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |█████████---------------------| 30.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |████████████------------------| 40.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███████████████---------------| 50.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████------------| 60.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |█████████████████████---------| 70.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |████████████████████████------| 80.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |███████████████████████████---| 90.0%\n",
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    }
   ],
   "source": [
    "from mhar.walk import walk\n",
    "X = walk(polytope=simplex,\n",
    "        X0 = x0,  \n",
    "        z=100, \n",
    "        T=10, \n",
    "        warm=0,\n",
    "        thinning=n**3, \n",
    "        device=device, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`walk`](https://sonder-art.github.io/mhar/walk.html#walk) produces `T` $\\times$ `z` uncorrelated points. It returns a vector of dimension `T` $\\times$ `z` $\\times$ `n`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0889, 0.0212, 0.4025,  ..., 0.5306, 0.0667, 0.5547],\n",
       "         [0.1682, 0.8578, 0.4345,  ..., 0.4050, 0.1121, 0.3566],\n",
       "         [0.7429, 0.1210, 0.1629,  ..., 0.0644, 0.8212, 0.0886]],\n",
       "\n",
       "        [[0.7528, 0.1165, 0.4401,  ..., 0.1687, 0.6633, 0.1609],\n",
       "         [0.1467, 0.6279, 0.3744,  ..., 0.6327, 0.1527, 0.7232],\n",
       "         [0.1004, 0.2556, 0.1854,  ..., 0.1986, 0.1840, 0.1159]],\n",
       "\n",
       "        [[0.0813, 0.0936, 0.2726,  ..., 0.1242, 0.0878, 0.2947],\n",
       "         [0.5696, 0.6319, 0.7036,  ..., 0.0086, 0.4037, 0.5547],\n",
       "         [0.3492, 0.2745, 0.0238,  ..., 0.8672, 0.5085, 0.1505]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6753, 0.3518, 0.0888,  ..., 0.6357, 0.7667, 0.1516],\n",
       "         [0.3004, 0.6083, 0.0235,  ..., 0.1452, 0.0366, 0.5145],\n",
       "         [0.0244, 0.0399, 0.8877,  ..., 0.2191, 0.1967, 0.3339]],\n",
       "\n",
       "        [[0.0865, 0.2683, 0.0859,  ..., 0.5572, 0.1254, 0.4516],\n",
       "         [0.7311, 0.5322, 0.0796,  ..., 0.0588, 0.0967, 0.2368],\n",
       "         [0.1824, 0.1995, 0.8346,  ..., 0.3841, 0.7779, 0.3116]],\n",
       "\n",
       "        [[0.5192, 0.7718, 0.6801,  ..., 0.2224, 0.0040, 0.0605],\n",
       "         [0.4667, 0.1371, 0.2536,  ..., 0.7400, 0.3262, 0.8919],\n",
       "         [0.0142, 0.0911, 0.0663,  ..., 0.0376, 0.6698, 0.0476]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some numerical inestability that causes the algorithm to degrade overtime, we recommend checking your walk everyonce in a while and be sure that it is not a big deal. The inestability is due to the projection matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite:  tensor(0)\n",
      "Nans:   tensor(0)\n",
      "Inequality violation:   tensor(927)\n",
      "Inequality violation with tol 1e-10:   tensor(0)\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-10\n",
    "print('Infinite: ', (~torch.isfinite(X)).sum())\n",
    "print('Nans:  ',(torch.isnan(X)).sum())\n",
    "print('Inequality violation:  ',(X.sum(1)!=1.0).sum())\n",
    "print(f'Inequality violation with tol {tol}:  ',((X.sum(1)- 1.0).abs()>tol).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sumamrize the steps taken we can use the `polytope_examples` for creating a [`Hypercube`](https://sonder-art.github.io/mhar/polytope_examples.html#hypercube)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/uumami/sonder-art/mhar/mhar/polytope.py:45: UserWarning:\n",
      "  The object will not create a copy of the tensors, so modifications will be reflected in the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mhar.polytope_examples import Simplex\n",
    "\n",
    "# Create a polytope (Simplex)\n",
    "simplex_sim = Simplex(10,\n",
    "                      dtype=torch.float64,\n",
    "                      device=device\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max non zero error for term (A A')^(-1)A at precision torch.float64:  tensor(2.2204e-16, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "simplex_sim.compute_projection_matrix(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define/Find inner points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simplex Status for the Chebyshev Center\n",
      " Optimization proceeding nominally.\n"
     ]
    }
   ],
   "source": [
    "x0_sim = ChebyshevCenter(polytope=simplex_sim, \n",
    "                    lb=None, \n",
    "                    ub=None, \n",
    "                    tolerance=1e-4,\n",
    "                    device=device,\n",
    "                    solver_precision=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number allowed -1.7976931348623157e+308\n",
      "Maximum number allowed 1.7976931348623157e+308\n",
      "Eps:  2.220446049250313e-16\n",
      "Values close to zero will be converted to 3eps or -3eps: 6.661338147750939e-16\n",
      "n:  10   mI: 10   mE: 1   z: 100\n",
      "% of burned samples |██----------------------------| 7.1%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of burned samples |██████████████████████████████| 100.0%\n",
      "% of iid samples |██████████████████████████████| 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1520e-01, 1.9804e-02, 7.7763e-02, 9.9372e-02, 1.0666e-01,\n",
       "          4.7353e-02, 2.4089e-02, 1.2750e-01, 3.4367e-02, 4.7841e-02,\n",
       "          6.7679e-02, 2.0295e-02, 9.1836e-02, 1.6823e-02, 1.8291e-02,\n",
       "          7.0606e-02, 5.0651e-02, 3.5442e-02, 1.4869e-03, 3.1108e-02,\n",
       "          4.8304e-02, 6.2594e-02, 2.1732e-01, 3.4688e-02, 5.0051e-02,\n",
       "          1.2728e-02, 1.1719e-01, 2.2064e-03, 1.3654e-02, 1.7988e-02,\n",
       "          5.2856e-02, 1.3055e-02, 6.6507e-02, 7.4944e-03, 2.8870e-02,\n",
       "          3.0513e-01, 2.7739e-02, 1.1810e-01, 1.0644e-01, 5.5130e-02,\n",
       "          1.9186e-01, 4.1276e-03, 2.2309e-01, 7.0959e-02, 1.5875e-02,\n",
       "          1.5771e-01, 7.4498e-03, 6.6981e-02, 9.2846e-02, 1.2573e-01,\n",
       "          9.0559e-02, 2.9251e-02, 3.5833e-01, 1.1234e-01, 1.9873e-01,\n",
       "          2.3755e-02, 7.5568e-04, 2.1011e-01, 6.4324e-03, 2.0992e-02,\n",
       "          1.4870e-01, 1.4210e-01, 2.6001e-02, 1.6850e-01, 9.5598e-03,\n",
       "          7.8793e-02, 1.8993e-01, 2.3360e-01, 3.3954e-01, 5.1940e-02,\n",
       "          4.8236e-01, 4.6770e-02, 9.2252e-02, 2.4227e-01, 2.6885e-02,\n",
       "          5.5988e-02, 5.2130e-02, 6.5577e-03, 7.4653e-02, 3.6497e-02,\n",
       "          1.0309e-01, 2.7669e-01, 3.9597e-02, 1.2605e-01, 2.1092e-01,\n",
       "          3.9481e-02, 1.1518e-02, 7.8954e-02, 5.0193e-02, 1.8436e-01,\n",
       "          1.8786e-01, 3.4861e-01, 1.0500e-01, 2.4956e-01, 1.1444e-02,\n",
       "          1.3071e-01, 1.8180e-01, 9.2854e-02, 1.1579e-02, 6.2876e-02],\n",
       "         [1.1732e-01, 6.0203e-03, 2.8216e-03, 1.0365e-01, 1.0955e-02,\n",
       "          7.1176e-02, 3.2881e-02, 3.4328e-01, 1.7465e-01, 1.9221e-01,\n",
       "          3.6379e-02, 2.1523e-01, 2.6973e-01, 3.4999e-01, 3.7247e-02,\n",
       "          2.9883e-01, 8.3997e-02, 2.6788e-01, 1.0243e-01, 3.0626e-01,\n",
       "          6.2901e-02, 5.9126e-02, 2.4796e-03, 4.7333e-02, 7.1346e-03,\n",
       "          8.3391e-02, 2.6385e-02, 2.6870e-01, 2.2617e-01, 1.3860e-01,\n",
       "          1.8633e-01, 4.7005e-02, 3.3859e-01, 1.2971e-01, 1.5655e-01,\n",
       "          2.6501e-02, 3.0599e-01, 8.2378e-02, 1.7185e-02, 2.3368e-02,\n",
       "          4.5034e-03, 6.0844e-03, 1.1600e-01, 1.6991e-01, 2.8657e-02,\n",
       "          1.1431e-02, 1.4085e-01, 3.0026e-02, 3.4520e-02, 6.6094e-02,\n",
       "          6.3149e-02, 6.0732e-02, 1.5279e-01, 1.1372e-01, 8.3449e-03,\n",
       "          2.8724e-01, 9.8592e-02, 7.8879e-02, 3.8037e-02, 3.8097e-02,\n",
       "          3.3600e-02, 2.2161e-01, 3.5476e-02, 1.1800e-01, 3.7689e-01,\n",
       "          1.3493e-01, 1.6147e-01, 1.3743e-01, 5.0601e-02, 2.0169e-01,\n",
       "          1.0531e-01, 8.2477e-02, 2.3473e-01, 1.7625e-02, 2.1074e-01,\n",
       "          2.3291e-01, 1.6421e-01, 9.9830e-02, 1.3988e-01, 1.2322e-01,\n",
       "          2.8879e-02, 1.1411e-01, 2.6213e-01, 9.0703e-02, 1.6237e-01,\n",
       "          1.5610e-01, 1.2930e-02, 4.6159e-02, 4.6877e-02, 2.7203e-02,\n",
       "          2.1843e-01, 1.0704e-01, 1.7013e-01, 2.1445e-02, 2.3797e-01,\n",
       "          2.2489e-01, 6.9380e-02, 5.1593e-02, 4.1673e-03, 1.8229e-01],\n",
       "         [3.7741e-02, 1.3599e-02, 9.2845e-02, 5.0604e-02, 1.7568e-01,\n",
       "          1.4746e-01, 1.0506e-01, 6.4526e-02, 2.9239e-01, 2.2158e-01,\n",
       "          2.5788e-02, 2.9854e-01, 7.0503e-02, 2.7666e-02, 5.2604e-02,\n",
       "          2.1490e-02, 2.2397e-02, 1.7406e-02, 2.9342e-01, 7.7148e-02,\n",
       "          2.2097e-02, 1.1792e-01, 9.1281e-02, 3.5598e-01, 3.6228e-02,\n",
       "          1.0279e-01, 1.4485e-01, 3.1391e-01, 1.1990e-02, 7.4669e-02,\n",
       "          1.4803e-01, 8.7013e-02, 4.4677e-02, 3.0667e-03, 1.5960e-02,\n",
       "          8.7949e-02, 9.6248e-02, 1.4113e-01, 4.9734e-02, 2.0384e-01,\n",
       "          9.1539e-02, 6.3023e-02, 7.7097e-02, 1.0578e-01, 2.7285e-01,\n",
       "          1.4996e-02, 8.7888e-02, 3.6542e-01, 1.3214e-01, 9.0136e-02,\n",
       "          9.8377e-02, 1.0995e-01, 2.2547e-01, 1.3902e-01, 1.3270e-01,\n",
       "          1.1464e-02, 1.5942e-01, 7.7017e-02, 2.2519e-01, 1.4753e-01,\n",
       "          5.1849e-02, 4.3538e-02, 4.7350e-04, 3.9691e-02, 1.1067e-01,\n",
       "          3.3255e-02, 1.8679e-03, 2.0241e-01, 2.0378e-02, 1.0853e-03,\n",
       "          4.4728e-02, 2.0378e-01, 3.3423e-02, 8.3269e-03, 2.6582e-03,\n",
       "          5.1881e-02, 3.1319e-02, 4.9918e-03, 5.5538e-02, 1.8517e-03,\n",
       "          3.3871e-01, 7.7213e-03, 5.0326e-02, 5.4550e-02, 5.0976e-02,\n",
       "          3.0379e-01, 1.4976e-02, 7.2932e-02, 9.8777e-03, 4.5814e-02,\n",
       "          1.4761e-01, 5.8636e-03, 9.5393e-02, 1.3186e-02, 4.0296e-01,\n",
       "          1.8033e-02, 2.4412e-02, 1.4313e-01, 2.1835e-01, 6.3904e-02],\n",
       "         [1.8559e-02, 6.2632e-03, 6.3321e-02, 1.5251e-01, 2.0290e-01,\n",
       "          2.2344e-01, 1.6259e-01, 9.6805e-02, 1.1066e-01, 3.1689e-02,\n",
       "          6.3664e-02, 9.7035e-02, 8.4973e-03, 3.2638e-02, 3.3033e-02,\n",
       "          8.8408e-03, 3.3775e-02, 3.7427e-02, 2.8798e-01, 4.4026e-02,\n",
       "          5.0291e-02, 1.2669e-02, 1.9412e-02, 1.1040e-01, 2.0107e-01,\n",
       "          6.1015e-02, 2.3159e-02, 8.9443e-03, 1.4407e-01, 1.8439e-01,\n",
       "          1.8126e-01, 8.9987e-02, 1.0353e-02, 1.7568e-03, 2.7576e-02,\n",
       "          1.2037e-01, 1.3604e-02, 3.3458e-02, 2.4171e-01, 7.9318e-02,\n",
       "          3.5007e-02, 2.8875e-03, 1.6090e-02, 8.9999e-03, 3.2558e-02,\n",
       "          3.8135e-02, 3.0808e-02, 1.7051e-02, 3.8245e-02, 6.8040e-02,\n",
       "          2.6039e-02, 4.9859e-01, 1.8985e-02, 2.7550e-02, 1.2441e-01,\n",
       "          3.2275e-02, 1.0943e-02, 1.0717e-01, 7.4638e-02, 1.8054e-01,\n",
       "          2.6161e-01, 3.6048e-02, 2.0392e-01, 6.4127e-02, 1.9720e-02,\n",
       "          5.1732e-02, 7.9827e-03, 1.6046e-01, 1.2006e-02, 1.2951e-01,\n",
       "          1.0007e-02, 1.2212e-01, 1.0358e-02, 6.3871e-02, 3.1131e-02,\n",
       "          1.4618e-02, 3.7878e-03, 3.2615e-01, 1.0008e-01, 4.8598e-01,\n",
       "          4.4201e-02, 2.2606e-03, 7.6345e-03, 1.2530e-01, 4.2059e-02,\n",
       "          1.8911e-01, 3.8809e-01, 1.6076e-01, 2.3086e-01, 1.3454e-02,\n",
       "          1.6558e-02, 1.9714e-02, 4.0101e-02, 1.5245e-01, 1.2369e-02,\n",
       "          1.3607e-01, 1.2751e-02, 1.0368e-01, 5.1909e-01, 1.2267e-01],\n",
       "         [1.0501e-01, 3.9312e-01, 7.8561e-03, 1.1405e-01, 2.1728e-01,\n",
       "          2.2990e-02, 1.0790e-02, 2.1549e-03, 9.2276e-03, 7.9180e-03,\n",
       "          1.3190e-01, 1.3693e-01, 9.9233e-02, 3.0380e-02, 2.4901e-02,\n",
       "          1.8787e-01, 3.7511e-01, 4.7839e-02, 7.0636e-02, 2.0361e-01,\n",
       "          5.5322e-02, 3.7308e-02, 1.6469e-01, 7.2669e-03, 7.2261e-02,\n",
       "          3.2279e-02, 9.3639e-02, 1.6325e-01, 8.8580e-02, 3.1826e-02,\n",
       "          7.8889e-02, 1.6606e-01, 1.8047e-01, 4.6407e-01, 1.8096e-01,\n",
       "          9.8785e-02, 4.2642e-02, 1.8363e-02, 1.9136e-01, 9.7498e-02,\n",
       "          2.5960e-02, 1.6421e-01, 2.1038e-01, 1.7461e-02, 2.0700e-01,\n",
       "          2.8977e-01, 1.2998e-01, 1.0695e-01, 8.1221e-02, 1.6061e-01,\n",
       "          2.1181e-02, 1.9240e-02, 2.4983e-02, 1.0613e-01, 1.1202e-02,\n",
       "          1.1573e-01, 1.1868e-01, 5.8499e-02, 3.0023e-02, 4.9783e-02,\n",
       "          8.2156e-03, 1.8833e-02, 8.1968e-02, 1.1507e-01, 6.0591e-03,\n",
       "          4.6841e-02, 7.2878e-02, 3.7531e-02, 1.2907e-01, 3.8696e-02,\n",
       "          6.7553e-02, 4.5215e-02, 2.1563e-01, 1.3498e-01, 1.7760e-01,\n",
       "          9.9415e-02, 5.7144e-02, 1.4238e-01, 4.5816e-03, 6.5244e-03,\n",
       "          1.6412e-03, 5.7189e-02, 1.8689e-01, 1.0755e-01, 1.8043e-02,\n",
       "          4.9186e-02, 1.9517e-01, 8.4092e-02, 2.3907e-01, 1.1630e-01,\n",
       "          1.2300e-01, 1.5197e-01, 7.9598e-02, 1.5167e-01, 8.6857e-02,\n",
       "          9.6195e-02, 1.9171e-01, 1.7479e-01, 2.7266e-02, 1.3326e-03],\n",
       "         [4.3111e-02, 3.0406e-01, 1.1589e-01, 3.7188e-01, 7.4454e-03,\n",
       "          2.6024e-02, 1.4489e-01, 1.1559e-01, 7.8037e-02, 7.5737e-02,\n",
       "          5.8174e-03, 2.6208e-02, 3.7532e-02, 1.1196e-01, 4.9959e-01,\n",
       "          4.2666e-02, 3.2597e-01, 1.7261e-01, 6.8312e-02, 1.1860e-01,\n",
       "          6.9027e-02, 1.5064e-01, 2.6812e-02, 1.4305e-01, 2.3201e-01,\n",
       "          2.1437e-02, 1.4792e-01, 6.1925e-03, 4.2360e-02, 9.2250e-02,\n",
       "          8.2749e-02, 1.7539e-01, 1.1344e-01, 7.8993e-02, 1.9649e-01,\n",
       "          4.5310e-02, 2.3427e-01, 2.5889e-01, 5.0295e-02, 4.4786e-04,\n",
       "          5.4323e-02, 3.0655e-01, 1.0711e-02, 1.7966e-02, 2.0797e-01,\n",
       "          3.2825e-01, 3.5855e-01, 5.7777e-02, 7.4462e-02, 1.0963e-01,\n",
       "          1.7767e-01, 9.8365e-03, 4.4471e-02, 1.8552e-01, 5.9484e-02,\n",
       "          4.1228e-02, 2.6638e-03, 2.5907e-01, 1.8446e-02, 1.8001e-01,\n",
       "          7.1227e-02, 4.9375e-02, 1.2731e-01, 1.8969e-01, 1.2160e-02,\n",
       "          8.6972e-02, 3.9171e-02, 2.7541e-03, 3.6587e-02, 1.4401e-01,\n",
       "          9.1558e-02, 1.3329e-01, 1.2907e-02, 1.0846e-01, 9.4682e-02,\n",
       "          3.1680e-02, 3.5527e-02, 1.1099e-01, 1.1425e-01, 4.7374e-02,\n",
       "          1.8818e-01, 1.6316e-01, 2.7692e-02, 6.7281e-02, 3.7478e-02,\n",
       "          1.5814e-01, 1.3310e-02, 1.2074e-01, 4.3944e-02, 1.9628e-01,\n",
       "          3.2678e-02, 1.6003e-02, 1.9353e-01, 1.8725e-02, 2.6767e-03,\n",
       "          5.1008e-02, 1.3689e-01, 1.5750e-02, 3.8494e-02, 2.0001e-01],\n",
       "         [1.8254e-02, 3.2457e-02, 6.7707e-02, 3.3506e-02, 5.4344e-02,\n",
       "          6.3571e-02, 1.8912e-01, 1.4547e-02, 2.4289e-02, 1.3824e-01,\n",
       "          1.2788e-01, 1.7346e-02, 9.4631e-02, 1.0819e-01, 1.3937e-01,\n",
       "          4.7767e-02, 6.9292e-03, 1.7086e-01, 4.8422e-02, 4.1030e-03,\n",
       "          1.9252e-01, 1.1880e-01, 6.4165e-02, 7.5359e-02, 6.6226e-02,\n",
       "          4.1074e-02, 2.0398e-02, 1.2225e-01, 7.3731e-02, 5.1480e-02,\n",
       "          2.4287e-02, 3.2170e-02, 1.1948e-02, 4.3357e-02, 2.4451e-02,\n",
       "          9.8813e-02, 6.8924e-02, 1.1156e-01, 2.1656e-02, 1.5373e-01,\n",
       "          3.7707e-02, 4.4615e-02, 2.2197e-01, 2.4466e-01, 3.0791e-02,\n",
       "          2.9794e-02, 6.0074e-03, 1.0404e-01, 1.0140e-01, 5.3190e-02,\n",
       "          1.3671e-02, 3.1700e-02, 2.5912e-02, 8.4963e-02, 2.4885e-01,\n",
       "          3.1885e-02, 1.5356e-01, 1.9486e-02, 1.8299e-01, 1.9346e-01,\n",
       "          2.8177e-01, 4.7691e-02, 3.2685e-02, 1.8528e-01, 1.3667e-01,\n",
       "          5.0818e-02, 2.8789e-01, 3.4620e-02, 1.0337e-01, 9.1024e-02,\n",
       "          8.6288e-02, 1.4706e-01, 1.4411e-01, 1.0635e-02, 9.1200e-03,\n",
       "          1.8575e-01, 4.8078e-01, 4.3997e-02, 1.8165e-01, 7.9679e-02,\n",
       "          1.4993e-01, 1.0888e-02, 1.2969e-02, 1.2755e-01, 1.6129e-01,\n",
       "          2.4854e-02, 9.4897e-02, 2.2092e-01, 1.4866e-01, 1.4182e-02,\n",
       "          9.2707e-02, 5.8941e-02, 7.5224e-02, 1.5887e-01, 3.8913e-02,\n",
       "          8.9434e-04, 9.3999e-02, 9.4290e-02, 1.1053e-01, 1.3522e-01],\n",
       "         [1.5479e-01, 1.4848e-02, 3.1828e-01, 1.4076e-02, 7.4570e-02,\n",
       "          1.6767e-01, 1.0166e-01, 1.2176e-02, 1.3610e-01, 5.9858e-02,\n",
       "          8.9117e-02, 1.2788e-01, 7.0382e-02, 6.3792e-02, 3.6383e-02,\n",
       "          1.5097e-01, 1.2133e-02, 7.1744e-02, 9.2671e-02, 6.4370e-02,\n",
       "          3.7880e-01, 2.5148e-01, 3.0544e-01, 1.8820e-02, 7.4788e-02,\n",
       "          5.3773e-01, 1.2778e-01, 2.5110e-02, 2.4498e-01, 1.1984e-01,\n",
       "          1.0980e-01, 1.1419e-01, 2.1121e-01, 5.8989e-02, 1.4724e-01,\n",
       "          4.1077e-02, 5.6250e-03, 1.1805e-02, 1.5716e-01, 3.7943e-01,\n",
       "          1.2927e-01, 8.2326e-03, 9.2305e-02, 7.7628e-02, 4.3165e-02,\n",
       "          6.0957e-03, 5.7567e-02, 8.4184e-02, 1.1670e-02, 1.0004e-01,\n",
       "          1.2229e-01, 7.4540e-02, 4.5156e-02, 1.7605e-01, 5.3424e-02,\n",
       "          1.4946e-01, 1.0277e-01, 4.8644e-02, 3.7931e-02, 1.0666e-01,\n",
       "          3.1671e-02, 1.5511e-01, 1.3019e-01, 4.1976e-02, 2.4147e-01,\n",
       "          2.5275e-03, 1.8064e-02, 5.0259e-02, 1.1369e-01, 2.2566e-01,\n",
       "          3.5377e-02, 5.4689e-02, 9.1494e-02, 1.4377e-01, 7.0745e-02,\n",
       "          1.5489e-01, 1.5718e-02, 4.3206e-02, 2.8505e-01, 1.2151e-01,\n",
       "          6.4775e-02, 2.0042e-03, 1.5745e-01, 8.5537e-02, 8.9939e-02,\n",
       "          5.4633e-02, 1.5930e-01, 1.3563e-01, 2.2314e-02, 6.2213e-02,\n",
       "          4.1247e-03, 2.0656e-01, 6.3557e-02, 2.0395e-02, 2.0807e-02,\n",
       "          1.5415e-02, 1.4966e-02, 9.7597e-03, 2.9884e-02, 2.3485e-03],\n",
       "         [2.1186e-01, 1.0984e-01, 1.3610e-02, 3.4897e-02, 7.7591e-02,\n",
       "          9.3049e-02, 7.8140e-02, 9.6434e-02, 9.7639e-02, 2.0292e-02,\n",
       "          1.9357e-01, 4.4003e-02, 1.5449e-01, 7.0218e-02, 1.3275e-01,\n",
       "          1.4316e-01, 7.0270e-02, 1.7049e-01, 7.8897e-03, 1.0548e-01,\n",
       "          4.1635e-02, 1.7746e-01, 5.1608e-02, 1.7604e-01, 1.5171e-01,\n",
       "          7.9338e-02, 2.7640e-02, 7.8997e-02, 1.2585e-01, 1.8608e-01,\n",
       "          7.4505e-02, 1.8116e-01, 2.0562e-02, 1.6089e-01, 1.1753e-01,\n",
       "          3.7931e-02, 1.3344e-01, 3.5273e-02, 1.2898e-02, 1.7291e-03,\n",
       "          3.7066e-02, 1.3346e-01, 2.2848e-02, 1.7647e-03, 1.3014e-01,\n",
       "          5.3839e-02, 3.5819e-02, 3.8815e-02, 4.5257e-02, 1.4335e-01,\n",
       "          1.2965e-01, 3.6741e-02, 8.9882e-02, 3.6534e-02, 8.8911e-02,\n",
       "          2.9582e-01, 1.8515e-01, 3.2567e-02, 3.5901e-01, 6.2718e-02,\n",
       "          7.7594e-02, 2.4407e-01, 4.1327e-02, 5.9989e-02, 3.8723e-02,\n",
       "          5.7344e-02, 1.3803e-01, 1.4439e-02, 1.0608e-01, 9.2652e-02,\n",
       "          3.5403e-03, 1.6459e-01, 1.3260e-01, 5.4083e-02, 1.5956e-01,\n",
       "          1.6585e-01, 8.5087e-02, 1.5320e-01, 1.2067e-02, 9.6520e-02,\n",
       "          7.9466e-02, 4.6986e-02, 9.3923e-02, 4.7243e-02, 1.1715e-02,\n",
       "          5.5678e-03, 2.2370e-02, 6.8649e-02, 9.5982e-02, 9.2381e-02,\n",
       "          1.1379e-01, 6.9524e-02, 1.7726e-01, 1.8119e-01, 1.2523e-02,\n",
       "          6.7930e-02, 1.3155e-01, 1.0126e-01, 2.5237e-02, 1.5250e-01],\n",
       "         [1.7815e-01, 9.9994e-02, 2.3991e-01, 2.5455e-02, 7.2572e-02,\n",
       "          1.3727e-01, 1.5078e-01, 1.2699e-01, 4.2640e-02, 2.0463e-01,\n",
       "          2.5821e-01, 1.6532e-02, 1.0317e-01, 1.8835e-01, 2.5835e-02,\n",
       "          2.7801e-02, 1.8764e-02, 8.2994e-03, 2.6750e-02, 4.5287e-02,\n",
       "          7.9099e-02, 1.1999e-02, 5.6805e-02, 3.1066e-02, 1.0851e-01,\n",
       "          2.8219e-02, 2.7103e-01, 1.0436e-02, 2.8626e-02, 1.0287e-01,\n",
       "          6.1289e-02, 9.3966e-02, 2.2446e-03, 5.1673e-02, 1.0437e-01,\n",
       "          1.3814e-01, 7.1530e-02, 1.8904e-01, 1.5156e-01, 5.5016e-03,\n",
       "          3.9276e-01, 2.6681e-01, 9.5059e-03, 2.8487e-01, 3.0991e-02,\n",
       "          6.9984e-02, 1.4508e-01, 1.2875e-01, 3.8824e-01, 8.3172e-02,\n",
       "          2.5742e-01, 1.2943e-01, 1.4019e-02, 1.8174e-02, 7.3949e-02,\n",
       "          1.1133e-02, 1.6746e-01, 1.0856e-01, 2.7304e-02, 2.0207e-02,\n",
       "          3.3760e-02, 4.1639e-02, 3.2065e-01, 1.7674e-02, 4.8080e-02,\n",
       "          4.5678e-01, 8.2711e-02, 1.2650e-01, 8.8676e-02, 2.3738e-02,\n",
       "          7.3270e-02, 7.5926e-06, 3.2493e-02, 2.1598e-01, 2.1688e-01,\n",
       "          7.0055e-03, 7.4293e-02, 6.8696e-02, 3.2261e-02, 8.3634e-04,\n",
       "          1.1261e-03, 3.1899e-01, 1.6139e-01, 1.6823e-01, 2.1521e-01,\n",
       "          1.9139e-02, 8.7436e-02, 1.1172e-02, 1.1223e-01, 2.4782e-01,\n",
       "          6.3246e-02, 1.5783e-02, 2.0044e-04, 3.2505e-02, 1.7348e-01,\n",
       "          2.5886e-01, 1.4254e-01, 2.1290e-01, 1.5403e-02, 7.6855e-02]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sim = walk(polytope=simplex_sim,\n",
    "        X0 = x0_sim,  \n",
    "        z=100, \n",
    "        T=1, \n",
    "        warm=0,\n",
    "        thinning=10000, \n",
    "        device= None, \n",
    "        seed=None,\n",
    "        verbosity=2\n",
    ")\n",
    "X_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite:  tensor(0)\n",
      "Nans:   tensor(0)\n",
      "Inequality violation:   tensor(927)\n",
      "Inequality violation with tol 1e-10:   tensor(0)\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-10\n",
    "print('Infinite: ', (~torch.isfinite(X)).sum())\n",
    "print('Nans:  ',(torch.isnan(X)).sum())\n",
    "print('Inequality violation:  ',(X.sum(1)!=1.0).sum())\n",
    "print(f'Inequality violation with tol {tol}:  ',((X.sum(1)- 1.0).abs()>tol).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
